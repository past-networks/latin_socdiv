---
title: "economic_complexity"
author: "Matteo Mazzamurro"
date: "2024-03-25"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction
This R markdown allows to reproduce the results of our study the economic complexity of the Roman empire (INSERT LINK TO PAPER).
The main input data are the (Latin) inscriptions containing occupational information.
The output are tables of occupation counts per geographic unit, before and after the correction of several potential sources of bias.

# Structure
The document is structured as follows:
* Preliminaries: 
  Loading the necessary packages and data.
* Data preparation: 
  - Defining consistent occupational units by standardising the spelling. 
  - Assign inscriptions to different geographical units: modern countries and Roman provinces.
* Table of occupations: 
  Compute the tables of occupational counts per geographical units.
* Uncertainty and bias mitigation
  - Define functions to mitigate data bias
  - Apply them to retrieve bias-corrected tables of occupations
* Economic complexity:
  Compute the economic complexity index for each geographical unit and the product complexity index for each occupation
* Assessment: 
  Here we study on effects of bias correction on occupation tables and economic complexity

# Preliminaries ----------------------------------------------------------------
## Packages
The following libraries are required.
```{r list required packages}
required_packages <- c("jsonlite",     # read json files
                       "arrow",        # read parquet files
                       "tidyverse",    # data cleaning and visualisation
                       "stringi",      # handle strings for spelling standardisation
                       "sf",           # load and handle spatial files
                       "raster",       # handle spatial files
                       "units",        # add physical units for geographic comparison
                       "ggspatial",    # additional plot features for spatial data
                       "triangulr",    # triangular distribution 
                       "Matrix")       # for eigenvalues
```

One needs to install them if they have not been installed yet.
```{r install missing packages}
packages_to_install <- required_packages[!(required_packages %in% installed.packages()[,"Package"])]
if(length(packages_to_install)) install.packages(packages_to_install)
```

Finally, one needs to load them.
```{r load required packages}
invisible(lapply(required_packages, library, character.only = TRUE))
```

Note that we use the following versions of the packages:
* jsonlite    : 1.8.7
* arrow       : 15.0.1
* tidyverse   : 2.0.0
* stringi     : 1.7.12
* sf          : 1.0.15
* raster      : 3.6.26
* units       : 0.8.2
* ggspatial   : 1.1.9
* triangulr   : 1.2.1
* Matrix      : 1.6.1

These were loaded in the R version 4.3.0.

To check your version of an installed package and R, you may use the following commands.
```{r check R and package version}
# R version
R.version

# example package version
packageVersion("tidyverse")
```

If a package has been updated since the release of this code, using the newer version may cause issues.
If the installed version is causing issues, one can try to solve them by installing the specific versions of the packages by uncommenting and running the following code. 
Note, however, that these older versions of the packages may not be available for newer versions of R, in which case it will be necessary to switch to an older version of R to run the code (please refer to the cran.r website on how to do that: https://cran.r-project.org/index.html).
```{r install and load specific package versions, if needed}
# #install devtools if not already installed
# if (!"devtools" %in% installed.packages()[,"Package"]) install.packages("devtools")
# 
# #load devtools
# library(devtools)
# 
# #install specific versions of the packages
# install_version("jsonlite", version = "1.8.7")
# install_version("arrow", version = "15.0.1")
# install_version("tidyverse", version = "2.0.0")
# install_version("stringi", version = "1.7.12")
# install_version("sf", version = "1.0.15")
# install_version("raster", version = "3.6.26")
# install_version("units", version = "0.8.2")
# install_version("ggspatial", version = "1.1.9")
# install_version("triangulr", version = "1.2.1")
# install_version("Matrix", version = "1.6.1")
# 
# #load packages
# invisible(lapply(required_packages, library, character.only = TRUE))
```

## Load Data
Read the data on inscriptions and standardised spelling
```{r read data on inscriptions}
# inscriptions
#inscriptions <- read_parquet("../../data/economic_complexity/LIST_occupsorgs_industry_simple_20231206.parquet") # this is the old data from before Petra's clean up, 511973 inscriptions
inscriptions <- read_parquet("../../data/large_data/LIST_occups_simple.parquet") # new data after Petra's cleanup, 525870 inscriptions

# occupations to merge 
occupations_all <- read_delim("../../data/economic_complexity/occupations.csv", delim=";")
occupations_to_merge <- read_delim("../../data/economic_complexity/occupations_to_merge.csv", delim=";")

# inscriptions containing people
EDH_people <- read_csv("../../data/economic_complexity/EDH_people_2021.csv")

# epitaphs
epitaphs <- inscriptions[inscriptions$type_of_inscription_auto %in% c("epitaph"),]

# greek inscriptions
greek_inscriptions <- read_parquet("../../data/large_data/GIST_v1-1.parquet") # manually downloaded from zenodo.org/records/10127597

```

Read the data on geographical units.
```{r geographical data}
# cities based on Hanson's dataset 
cities <- st_read("../../data/economic_complexity/roman_cities_pop.geojson")

# Roman provinces (digitised and corrected by Adam PaÅ¾out)
provinces <- read_sf("../../data/economic_complexity/provinces/provinces.shp")

# country borders
## Orginal data can be download from https://public.opendatasoft.com/api/explore/v2.1/catalog/datasets/world-administrative-boundaries/exports/shp 
world_sf <- read_sf('../../data/economic_complexity/world-administrative-boundaries/world-administrative-boundaries.shp') 
## dissolve the administrative boundaries for visualisation purposes
world_no_borders_sf <- st_union(world_sf) 
```

## Shortcuts
Create the following function to plot and save a ggplot as pdf and png
```{r function to print and save ggplots}
plot_and_save <-  function(ggp, title=deparse(substitute(ggp)), width = 7, height = 5.5){
  # print the plot
  #print(ggp)
  
  # save the plot (deafault title is the name of the file)
  ggsave(paste0("../../figures/economic_complexity/",title,".pdf"), 
         ggp, width = width, height = height)
  #ggsave(paste0("../../figures/economic_complexity/",title,".jpeg"), 
  #       ggp, width = width, height = height, dpi = 300)
}
```

The following function takes as input a data frame or table and returns what kind of geographical units (gu) is included. If the df does not contain this info, it throws an error.
```{r function to detect geographical units}
which_gu <- function(d){
  if (is.data.frame(d)){
    # ensure that the info on geographical unit (gu) is included in the input
    if ("province" %in% colnames(d)){
      gu <- "province"
    } else if ("country" %in% colnames(d)){
        gu <- "country"
    } else {
      stop("The input dataframe does not contain info on the geographical unit")
    }
  } else if (is.table(d)){
    # check that the table contains a couple of names of provinces or countries (to avoid problem with future bootstrapping)
    if (any(c("Achaia", "Aegyptus") %in%  rownames(d))){
      gu <- "province"
    } else if (any(c("ITA", "FRA") %in%  rownames(d))){
      gu <- "country"
    } else {
      stop("The input table does not contain info on the geographical unit")
    }
  } else {
    stop("The input is neither a table nor a dataframe")
  }
  return(gu)
}
```

The following function takes a table as input and sorts its rows and columns based on the row sums and column sums respectively, before returning it as an output.
```{r sort table function}
sort_table <- function(tb, decreasing = TRUE){

  # col names sorted by sums
  col_names <- names(
    sort(
      colSums(tb),
      decreasing=decreasing)
    )
  # row names (sorted by sums)
  row_names <- names(
    sort(
      rowSums(tb),
      decreasing=decreasing)
    )
  
  # apply sorting
  tb <- tb[row_names,col_names]
  return(tb)
}
```

# Data preparation -------------------------------------------------------------
## Occupational units
First, select only the inscriptions containing information on occupations
```{r select inscriptions with occupations}
inscriptions_w_occupations <- inscriptions[inscriptions$occups_N>0,]
```

Identify alternative spellings for the same occupation
```{r identify alternative spellings}
occupations_to_merge_min <- occupations_to_merge[,c("Main_term","To_be_merged")]

occupations_to_merge_separated <- occupations_to_merge_min %>%
  separate_rows(To_be_merged, sep = ", ") %>%
  mutate(To_be_merged = trimws(To_be_merged)) # Remove leading/trailing spaces if any

# Rename columns if needed
colnames(occupations_to_merge_separated) <- c("standard_spelling", "alternative_spellings")
```

Add a standard spelling column to the occupations. TO CHECK
```{r standardise occupation spelling TO CHECK}
# ensure no unwanted space are there in the terms
occupations_all$Term <- trim(occupations_all$Term)

# find full names of occupations by attaching Term2 if present
occupations_all$Term_full <- ifelse(!is.na(occupations_all$Term2),
                                    paste(occupations_all$Term,occupations_all$Term2),
                                    occupations_all$Term)
                                           
#initialise column of standard spelling 
occupations_all$std_spelling <- NA

for (i in seq(nrow(occupations_all))) {
  occ_term <- occupations_all$Term_full[i]
   # CHECK THIS
  occupations_all$std_spelling[i] <- ifelse(occ_term %in% occupations_to_merge_separated$alternative_spellings,
                                            occupations_to_merge_separated$standard_spelling[which(occupations_to_merge_separated$alternative_spellings==occ_term)],
                                            occ_term
                                            )
}

# select only necessary columns
occupations_dictionary <- unique(occupations_all[,c("Term_full","std_spelling")]) %>%
  setNames(c("Term","std_spelling"))

# initialise new column of standard spelling for occupations in inscriptions
inscriptions_w_occupations$occups_std <- rep(list(NA),nrow(inscriptions_w_occupations))

for (i in seq(nrow(inscriptions_w_occupations))){
  # extract an inscription
  inscription <- inscriptions_w_occupations[i,]
  # find the occupations in the inscription and associate it with the standard spellings
  inscription_occ <- data.frame(Term=unlist(inscription$occups)) %>%
    left_join(occupations_dictionary,
              by = "Term")
  # save the standard spellings for the occupations in the inscription
  inscriptions_w_occupations$occups_std[[i]] <- inscription_occ$std_spelling
}
```

Extract info on unique occupations in inscriptions to fill the table
```{r unique occupations for online table}
unique_occupations_in_inscriptions_df <- data.frame(std_spelling = sort(unique(unlist(inscriptions_w_occupations$occups_std)))) %>% 
  left_join(occupations_all,by=join_by(std_spelling),)

write.csv(unique_occupations_in_inscriptions_df,"../../results/economic_complexity/unique_occupations_in_inscriptions.csv")
```

## Geographical units
### Intro
We assign the inscriptions to geographical units. 
Note that the computation of the economic complexity indices requires each geographical unit to include at least one inscription.
However, the data is very sparse.
Hence, naive methods such as using a cell raster will not work.
Therefore, we will only use larger geographical units: modern countries and ancient Roman provinces.

As a first step, convert the inscriptions into sf files
```{r create sf files of inscriptions}
# all (geolocated) Latin inscriptions
latin_inscriptions_sf  <- st_as_sf(na.omit(inscriptions[,c("LIST-ID","Longitude","Latitude")]),
                                  coords = c("Longitude","Latitude"),
                                  crs = crs("epsg:4326"))

# greek inscriptions (See if more columns will be necessary)
greek_inscriptions_sf <- st_as_sf(na.omit(greek_inscriptions[,c("PHI_ID","Longitude","Latitude")]),
                                  coords = c("Longitude","Latitude"),
                                  crs = crs("epsg:4326"))

# Latin inscriptions with occupations

# only those with Latitude and Longitude
inscriptions_w_occupations_geo <- inscriptions_w_occupations %>% 
  filter(!is.na(Latitude))


inscriptions_w_occupations_sf <- st_as_sf(inscriptions_w_occupations_geo,
                                          coords = c("Longitude","Latitude"),
                                          crs = crs("epsg:4326"))

# Latin epitaphs

# only those with Latitude and Longitude

epitaphs_geo <- epitaphs %>% 
  filter(!is.na(Latitude))

epitaphs_sf <- st_as_sf(epitaphs_geo,
                        coords = c("Longitude","Latitude"),
                        crs = crs("epsg:4326"))
```

We consider two sets of geographical units: contemporary countries and ancient Roman provinces (at the time of Trajan).

### Inscriptions by modern countries
#### Functions
First, find a list of the countries that intersected the Roman Empire
```{r countries intersection the Roman Empire}
intersecting_indices <- st_intersects(world_sf, provinces, sparse = FALSE)
intersecting_countries <- world_sf$iso3[apply(intersecting_indices, 1, any)]
```

The following function takes the sf of inscriptions and assigns each one to the corresponding (closest) modern country
```{r function assigning inscriptions to modern countries}
# function to assign countries to the Roman Empire
inscriptions_to_country_f <- function(insc_sf){
  # extract the coordinates
  insc_coordinates <-  data.frame(st_coordinates(insc_sf)) %>% setNames(c("Longitude","Latitude"))
  
  # find closest country
  inscr_closest_country <- world_sf$iso3[st_nearest_feature(insc_sf,world_sf)] 
  
  # create sf of inscriptions with closest countries (and separate columns for coordinates for convenient plotting)
  inscr_in_countries <- cbind(insc_sf,
                              country = inscr_closest_country,
                              insc_coordinates)
  
  # select only those countries that intersect the empire
  inscr_in_countries <- inscr_in_countries[inscr_in_countries$country %in%intersecting_countries,]
  
  # return 
  return(inscr_in_countries)
}
```

The following function produces a map of inscriptions classified by geographical unit
```{r function to create map of inscriptions by country}
inscriptions_to_gu_map_f <- function(inscr_in_gu){
  
  # ensure that the info on geographical unit (gu) is included in the input
  gu <- which_gu(inscr_in_gu)
  
  # base map
  inscriptions_by_gu_plot <- ggplot() +
    # base map
    geom_sf(data = world_no_borders_sf) +
    # add inscriptions
    geom_sf(data = inscr_in_gu,
            aes(color = inscr_in_gu[[gu]])) +
    # define theme, window, and style
    theme_minimal() + 
    coord_sf(
      xlim = c(min(inscr_in_gu$Longitude), 
               max(inscr_in_gu$Longitude)), 
      ylim = c(min(inscr_in_gu$Latitude), 
               max(inscr_in_gu$Latitude))) +
      guides(color=guide_legend(title = gu)) + 
    annotation_north_arrow(
      location = "bl", 
      which_north = "true",
      pad_x = unit(0.4, "in"), pad_y = unit(0.4, "in"),
      style = ggspatial::north_arrow_nautical(
        fill = c("grey40", "white"),
        line_col = "grey20",
    )
  ) 
}
```

#### Classify and plot inscriptions by country 
We now associate the above sets of inscriptions (all Latin inscriptions, Greek inscriptions, Latin inscriptions with occupations and epitaphs) to their country and plot them.

First we consider all (Latin) inscriptions.
```{r all Latin inscriptions}
# associate inscriptions with the country
latin_inscriptions_in_countries <- inscriptions_to_country_f(latin_inscriptions_sf)

# create, save and print the map
latin_inscriptions_in_countries_map <- inscriptions_to_gu_map_f(latin_inscriptions_in_countries)
plot_and_save(latin_inscriptions_in_countries_map)
```

Then, we consider inscriptions with occupations.
```{r inscriptions with occupations}
# associate inscriptions with the country
inscriptions_w_occupations_in_countries <- inscriptions_to_country_f(inscriptions_w_occupations_sf)

# create and print the map
inscriptions_w_occupations_in_countries_map <- inscriptions_to_gu_map_f(inscriptions_w_occupations_in_countries)
plot_and_save(inscriptions_w_occupations_in_countries_map)
```

Then, we consider epitaphs.
```{r epitaphs}
# associate inscriptions with the country
epitaphs_in_countries <- inscriptions_to_country_f(epitaphs_sf)

# create and print the map
epitaphs_in_countries_map <- inscriptions_to_gu_map_f(epitaphs_in_countries)
plot_and_save(epitaphs_in_countries_map)
```

Finally, we consider Greek inscriptions
```{r Greek inscriptions}
# associate inscriptions with the country
greek_inscriptions_in_countries <- inscriptions_to_country_f(greek_inscriptions_sf)

# create and print the map
greek_inscriptions_in_countries_map <- inscriptions_to_gu_map_f(greek_inscriptions_in_countries)
plot_and_save(greek_inscriptions_in_countries_map)
```

Count them
```{r counts of inscriptions by countries}
# all latin inscriptions
n_latin_inscriptions_in_countries <- latin_inscriptions_in_countries$country %>% 
  table() %>% data.frame() %>% setNames(c("country","n_latin_inscriptions"))
# latin inscriptions with occupations
n_inscriptions_w_occupations_in_countries <- inscriptions_w_occupations_in_countries$country %>% 
  table() %>% data.frame() %>% setNames(c("country","n_inscriptions_w_occupations"))
# greek inscriptions 
n_greek_inscriptions_in_countries <- greek_inscriptions_in_countries$country %>% 
  table() %>% data.frame() %>% setNames(c("country","n_greek_inscriptions"))
```

### Inscriptions by roman provinces
#### Function
We define an analogous function for assigning inscriptions to ancient Roman provinces (at Trajan's time).
In this case, a few preliminary notes are needed:
* Province borders were constantly shifting, so this choice is in principle not more meaningful than using modern country borders.
* Several inscriptions were found outside of the borders of the Roman Empire.
* The province boundaries in our shapefile are approximate, with certain major areas missing (e.g., the Balearic Islands), which we manually adjust.

The following function takes the sf of inscriptions and assigns each one to the corresponding province. 
```{r function assigning inscriptions to provinces}
inscriptions_to_province_f <- function(insc_sf){
  # find name of the id column (changes between Latin and Greek inscriptions)
  id_col <- ifelse("LIST-ID" %in% colnames(insc_sf),"LIST-ID","PHI_ID")
  
  # extract the coordinates
  insc_coordinates <-  data.frame(st_coordinates(insc_sf)) %>% setNames(c("Longitude","Latitude"))
  
  # assign inscription to province if it falls directly inside 
  insc_in_provinces <-  st_join(insc_sf, provinces)
  
  # in the very rare cases when inscriptions fall on the border between two provinces, assign them to one of them 
  insc_in_provinces <- insc_in_provinces[which(!duplicated(insc_in_provinces[[id_col]])),]

  # add columns of coordinates
  insc_in_provinces$Latitude <- insc_coordinates$Latitude
  insc_in_provinces$Longitude <- insc_coordinates$Longitude

  # identify unassigned inscriptions and define sf
  unassigned_insc_index <- which(is.na(insc_in_provinces$province))
  unassigned_insc <- insc_sf[unassigned_insc_index,]
  unassigned_insc_sf <- st_as_sf(unassigned_insc,
                                 coords = c("Longitude","Latitude"),
                                 crs = crs("epsg:4326"))
    
  # find list of unique locations (to speed up the computation of distance)
  unassigned_insc_unique_locations_sf <- unassigned_insc_sf[,"geometry"] %>% unique()
  
  # find the closest province to each unassigned location
  unassigned_insc_unique_locations_sf$closest_province_index <- st_nearest_feature(
    unassigned_insc_unique_locations_sf,
    provinces
    )
 
   # compute distance to the closest province
  unassigned_insc_unique_locations_sf$distance_to_province <- st_distance(
    unassigned_insc_unique_locations_sf,
    provinces[unassigned_insc_unique_locations_sf$closest_province_index,],
    by_element = TRUE
  )
  
  # transform unassigned inscriptions in df
  unassigned_insc_df <- unassigned_insc_sf
  unassigned_insc_df[,c("Longitude","Latitude")] <- st_coordinates(unassigned_insc_sf)
  unassigned_insc_df <- unassigned_insc_df %>% st_drop_geometry()
  
  # transform unique locations in df 
  unassigned_insc_unique_locations <- unassigned_insc_unique_locations_sf 
  unassigned_insc_unique_locations[,c("Longitude","Latitude")] <-
    st_coordinates(unassigned_insc_unique_locations_sf)
  unassigned_insc_unique_locations <- unassigned_insc_unique_locations %>% st_drop_geometry()
  
  # merge info to retrieve full list of unassigned inscriptions with distances to nearest province
  unassigned_insc_df <- unassigned_insc_df %>% left_join(
    unassigned_insc_unique_locations,
    by = c("Longitude","Latitude")
  )
  
  # retrieve name of the province for greater readability
  unassigned_insc_df$closest_province <-  provinces$province[unassigned_insc_df$closest_province_index]

  # automatically assign a point to a province if the distance is smaller than 50km.
  insc_in_provinces$province[unassigned_insc_index] <- ifelse(
    unassigned_insc_df$distance_to_province < set_units(50000,"meters"),
    unassigned_insc_df$closest_province,
    NA
    )
  
  # deal manually with cases of provinces with nearby islands
  for (p in c("Hispania Citerior","Achaia","Creta","Sicilia","Asia")) {
    province_inscriptions_id <- unassigned_insc_df[unassigned_insc_df$closest_province==p,id_col]
    insc_in_provinces$province[insc_in_provinces[[id_col]] %in% province_inscriptions_id[[id_col]]] <- p
  }
  
  # finally, delete outliers
  insc_in_provinces <- insc_in_provinces[!is.na(insc_in_provinces$province),]
  
  # return 
  return(insc_in_provinces)
}
```

#### Classify and plot inscriptions by province 
We now associate the above sets of inscriptions (all Latin inscriptions, Greek inscriptions, Latin inscriptions with occupations and epitaphs) to their province.
```{r associate inscriptions with province}
latin_inscriptions_in_provinces <- inscriptions_to_province_f(latin_inscriptions_sf)
inscriptions_w_occupations_in_provinces <- inscriptions_to_province_f(inscriptions_w_occupations_sf)
epitaphs_in_provinces <- inscriptions_to_province_f(epitaphs_sf) 
greek_inscriptions_in_provinces <- inscriptions_to_province_f(greek_inscriptions_sf)
```

Finally, we plot them.
```{r plot map of inscriptions in provinces}
# create the maps
latin_inscriptions_in_provinces_map <- inscriptions_to_gu_map_f(latin_inscriptions_in_provinces)
inscriptions_w_occupations_in_provinces_map <- inscriptions_to_gu_map_f(inscriptions_w_occupations_in_provinces)
epitaphs_in_provinces_map <- inscriptions_to_gu_map_f(epitaphs_in_provinces)
greek_inscriptions_in_provinces_map <- inscriptions_to_gu_map_f(greek_inscriptions_in_provinces)

# plot and save maps
plot_and_save(latin_inscriptions_in_provinces_map)
plot_and_save(inscriptions_w_occupations_in_provinces_map)
plot_and_save(epitaphs_in_provinces_map)
plot_and_save(greek_inscriptions_in_provinces_map)
```

Find counts of all Latin inscriptions by province
```{r counts of inscriptions by province}
# all latin inscriptions
n_latin_inscriptions_in_provinces <- latin_inscriptions_in_provinces$province %>% 
  table() %>% data.frame() %>% setNames(c("province","n_latin_inscriptions"))
# latin inscriptions with occupations
n_inscriptions_w_occupations_in_provinces <- inscriptions_w_occupations_in_provinces$province %>% 
  table() %>% data.frame() %>% setNames(c("province","n_inscriptions_w_occupations"))
# greek inscriptions 
n_greek_inscriptions_in_provinces <- greek_inscriptions_in_provinces$province %>% 
  table() %>% data.frame() %>% setNames(c("province","n_greek_inscriptions"))
```

### Estimate population by geographical unit
#### Intro
A major obstacle in reconstructing the occupational structure of the Roman Empire is the lack of precise data on population size and distribution.
Our inscription-based data sets offer a cumulative view of occupations over the entire duration of the Empire.
If the rate of inscription production were constant across individuals, we would expect the total number of inscriptions in a region to be proportional to the total number of person-years lived in that region while it was under Roman rule.
Here, we aim to reconstruct that number in order to provide a more accurate estimate of the occupational structure.

As far as we know, there are no estimates of the Ancient Roman population at the provincial level (and certainly not within the borders of modern nations or town hinterlands).
Instead, a set of estimates exist for the population in 10 macroregions of the Empire (Table 3.1 in "Scheidel W. Demography. In: Scheidel W, Morris I, Saller RP, eds. The Cambridge Economic History of the Greco-Roman World. Cambridge University Press; 2007:38-86") and for the population of individual urban units (Hanson).

For each macroregion, we compute the (peak) urban population by summing the population estimates of all the urban areas falling within it.
We subtract the urban population to obtain an estimate of the rural population in the macroregion. 

To each province in the macroregion, we find the urban population by summing the population estimates of the urban areas falling within it. We find the rural population by taking a part of the rural population of the macroregion, proportionate to the area of the province.
Note that this is the same as assuming that the rural population density is approximately constant within a macroregion. This is a crude assumption, but the most natural one that can be made with limited data and without additional population models. In the future, it can be improved by integrating additional information (e.g., terrain and suitability to agriculture).

To estimate population in each year before and after the peak assume naively a constant annual population growth/decline rate r. We take r distributed according to a triangular distribution between 0 and 0.5%, with a peak at 0.15%, following Hanson's estimates for urban population (Hanson, J., 2021. New approaches to the urban population and urbanization rate of the Roman Empire, AD 1 to 200.). 

We sampled r 1,000 times to generate 1,000 different sets of population curves, with each set representing a possible reconstruction of the maximum and minimum urban, rural, and total population for the provinces of the Roman Empire.

For each province, we integrated these population curves over the period during which the province was part of the Roman Empire. 
This allowed us to estimate total number of person-years lived in the province under Roman rule. 

#### Provinces
First, associate each province to the macroregion mentioned by Scheidel.
```{r assign each province to its macroregion}
provinces_macroregions_list <- list(
  Italy_and_islands = c(
    "Aemilia (Regio VIII)",
    "Apulia et Calabria (Regio II)",
    "Bruttium et Lucania (Regio III)",
    "Corsica",
    "Etruria (Regio VII)",
    "Latium et Campania (Regio I)",
    "Liguria (Regio IX)",
    "Picenum (Regio V)",
    "Samnium (Regio IV)",
    "Sardinia",
    "Sicilia",
    "Transpadana (Regio XI)",
    "Umbria (Regio VI)",
    "Venetia et Histria (Regio X)"
  ),
  Iberia = c(
    "Baetica",
    "Hispania Citerior",
    "Lusitania"
    ),
  Gaul_and_Germany = c(
    "Alpes Cottiae",
    "Alpes Graiae",
    "Alpes Maritimae",
    "Alpes Poeninae",
    "Aquitania",
    "Belgica",
    "Germania Inferior",
    "Germania Superior",
    "Lugdunensis",
    "Narbonensis",
    "Raetia"
    ),
  Britain = c(
    "Britannia"
    ),
  Danubian_region = c(
    "Dacia",
    "Dalmatia",
    "Moesia Inferior",
    "Moesia Superior",
    "Noricum",
    "Pannonia Inferior",
    "Pannonia Superior"
    ),
  Greek_peninsula = c(
    "Achaia",
    "Creta",
    "Epirus",
    "Macedonia",
    "Thracia"
    ),
  Anatolia = c(
    "Asia",
    "Bithynia et Pontus",
    "Cappadocia",
    "Cilicia",
    "Lycia et Pamphylia",
    "Galatia"
    ),
  Greater_Syria = c(
    "Arabia",
    "Cyprus",
    "Iudaea",
    "Syria"
    ),
  Egypt = c(
    "Aegyptus"
    ),
  North_Africa = c(
    "Africa Proconsularis",
    "Cyrene",
    "Mauretania Caesariensis",
    "Mauretania Tingitana",
    "Numidia")
)

# transform to dataframe
provinces_macroregions_df<-data.frame(unlist(provinces_macroregions_list)) %>% setNames("province")
provinces_macroregions_df$macroregion <- provinces_macroregions_df %>% rownames() %>% str_remove_all("[0-9]") %>% str_replace_all("_"," ")
rownames(provinces_macroregions_df) <- NULL
```

Compute the area of each province, and the area of each macroregion
```{r areas of provinces and macroregions}
# areas of the provinces
provinces_areas_df <- data.frame(province = provinces$province,
                                 area = st_area(provinces))

provinces_macroregions_areas_df <- left_join(provinces_macroregions_df,
                                       provinces_areas_df)

# areas of the macroregions
macroregions_areas_df <- provinces_macroregions_areas_df %>%
  group_by(macroregion) %>%
  summarize(macroregion_area = sum(area))
```

Find the proportion of the area of each macroregion falling in a specific province
```{r province area/macroregion area}
# add to the province and macroregions df the areas of the macroregions
provinces_macroregions_full_info_df <- left_join(
  provinces_macroregions_areas_df,
  macroregions_areas_df,
  by = join_by(macroregion))

# find the proportion of the macroregion's area falling in each province
provinces_macroregions_full_info_df$area_proportion <- as.numeric(
  provinces_macroregions_full_info_df$area/provinces_macroregions_full_info_df$macroregion_area)
```

Compute the urban population for each province.
```{r urban population by province}
# assign each city to its closest province 
# RK: the province may differ from those used by Hanson, and we need this step to ensure consistency.
closest_province_to_city_index <- st_nearest_feature(cities,provinces)
cities_in_provinces_df <- data.frame(
  Primary.Key = cities$Primary.Key,
  Ancient.Toponym = cities$Ancient.Toponym,
  pop_est = cities$pop_est,
  province = provinces$province[closest_province_to_city_index]
  )

# compute the urban population by province
province_urban_population_df <- cities_in_provinces_df %>%
  group_by(province) %>%
  summarize(urban_pop = sum(pop_est)) 
```

Assign urban population 0 to provinces without cities (e.g., Alpes Graiae).
```{r provinces without cities}
# add a row with value 0 
province_urban_population_df <- rbind(
  province_urban_population_df,
  data.frame(
    province = provinces$province[
      which(!provinces$province %in%
              province_urban_population_df$province)],
    urban_pop = 0
    )
)

# sort alphabetically
province_urban_population_df <- province_urban_population_df[order(province_urban_population_df$province),]
```

Finally compute the urban population by macroregion.
```{r urban population by macroregion}
# urban population of the macroregions
macroregions_urban_population_df <- province_urban_population_df %>%
  left_join(provinces_macroregions_areas_df) %>%
  group_by(macroregion) %>%
  summarize(urban_pop = sum(urban_pop))

#RK: HANSON'S CITY POPULATION DATA SHOULD CONTAIN MORE INFO ON THE INHERENT UNCERTAINTY
```

Then digitise the total population estimates for each macroregion.
```{r manually encode population estimates for the macroregions}
# Data based on "Scheidel W. Demography. In: Scheidel W, Morris I, Saller RP, eds. The Cambridge Economic History of the Greco-Roman World. Cambridge University Press; 2007:38-86." Table 3.1
macroregion_pop_165AD_df <- data.frame(
  macroregion = c("Italy and islands", 
                  "Iberia", 
                  "Gaul and Germany",
                  "Britain",
                  "Danubian region",
                  "Greek peninsula",
                  "Anatolia",
                  "Greater Syria",
                  "Egypt",
                  "North Africa"),
  tot_pop_min = c(12000000,
                  7000000,
                  9000000,
                  1500000,
                  5000000,
                  2500000,
                  9000000,
                  5000000,
                  5000000,
                  7000000),
  tot_pop_max = c(13000000,
                  9000000,
                  12000000,
                  2000000,
                  6000000,
                  3000000,
                  10000000,
                  6000000,
                  6000000,
                  8000000))
```

It is easy to have estimates of the rural population of each macroregion by subtracting the urban population from the estimates of the urban population.
```{r total, rural urban and rural population of the macroregions}
# total and urban population 
macroregion_pop_165AD_rural_urban_df <- left_join(
  macroregion_pop_165AD_df,
  macroregions_urban_population_df)

# rural population
macroregion_pop_165AD_rural_urban_df$rural_pop_min <- macroregion_pop_165AD_rural_urban_df$tot_pop_min - macroregion_pop_165AD_rural_urban_df$urban_pop
macroregion_pop_165AD_rural_urban_df$rural_pop_max <- 
macroregion_pop_165AD_rural_urban_df$tot_pop_max - macroregion_pop_165AD_rural_urban_df$urban_pop
```

To estimate the rural population in each province, we distribute the rural population living in each macroregion amongst the provinces that form it, proportionately to their area.
```{r urban, rural and total population by province}
# add the province's urban population to the full info df
provinces_macroregions_full_info_df <- left_join(
  provinces_macroregions_full_info_df,
  province_urban_population_df
)

# add the populations of the macroregion
provinces_macroregions_full_info_df <- left_join(
  provinces_macroregions_full_info_df,
  # add prefix macroregion to the column names to avoid confusion
  macroregion_pop_165AD_rural_urban_df %>% 
    setNames(c("macroregion",
               paste0(
                 "macroregion_",
                 colnames(macroregion_pop_165AD_rural_urban_df)[-1])))
)

# add the rural population proportionately to the area of the province
provinces_macroregions_full_info_df$rural_pop_max <- 
  provinces_macroregions_full_info_df$macroregion_rural_pop_max*provinces_macroregions_full_info_df$area_proportion
provinces_macroregions_full_info_df$rural_pop_min <- 
  provinces_macroregions_full_info_df$macroregion_rural_pop_min*provinces_macroregions_full_info_df$area_proportion

# compute the min/max total population for each province
provinces_macroregions_full_info_df$tot_pop_min <- provinces_macroregions_full_info_df$urban_pop+
  provinces_macroregions_full_info_df$rural_pop_min
provinces_macroregions_full_info_df$tot_pop_max <- provinces_macroregions_full_info_df$urban_pop+
  provinces_macroregions_full_info_df$rural_pop_max
```

Check that total values of the estimates make sense
```{r check total population in macroregion}
# compare these two data frames with the population in macroregion_pop_165AD_df
provinces_macroregions_full_info_df %>% group_by(macroregion) %>%
  summarize(pop = sum(tot_pop_min))
provinces_macroregions_full_info_df %>% group_by(macroregion) %>%
  summarize(pop = sum(tot_pop_max))
```

Extract only population info per each province
```{r select population columns from full info df}
provinces_population_df <-
  provinces_macroregions_full_info_df[,c("province",
    "urban_pop",
    "rural_pop_min",
    "rural_pop_max",
    "tot_pop_min",
    "tot_pop_max")]

# set row names
rownames(provinces_population_df) <- provinces_population_df$province 
```

Then compute the ratios of rural population
```{r rural population ratios in provinces}
provinces_rural_pc_df <- data.frame(
  province = provinces_population_df$province,
  "min" = provinces_population_df$rural_pop_min/provinces_population_df$tot_pop_min,
  "max" = provinces_population_df$rural_pop_max/provinces_population_df$tot_pop_max
)
```

Finally save these results.
```{r save population by province as csv}
#write.csv(provinces_macroregions_full_info_df,
#          "../../results/economic_complexity/provinces_macroregions_full_info.csv")
#write.csv(provinces_population_df,
#          "../../results/economic_complexity/provinces_population.csv")

```

#### Countries
We perform a similar operation to estimate the urban and rural population who lived within the borders of modern day countries.
We estimate the urban population using Hanson's dataset. 
We estimate the rural population by distributing the above estimates of rural populations proportionately to the percentage of area of a province falling in each country.
E.g. The province "Sicilia" falls 100% within the modern country of Italy, hence we assign all of Sicilia's rural population to Italy. Instead if another province falls 50% in in a country and 50% in another country, we assign half of the province's rural population to the each country.
```{r country and provinces sf}
# add area info to countries
world_sf$area <- st_area(world_sf)

# find intersections between the two shapefiles of modern day countries and provinces
intersections <- st_intersection(world_sf,provinces) %>%
  dplyr::select(c("iso3","area","province","geometry")) %>% # select only needed columns
  setNames(c("country","country_area","province","geometry"))
```

Compute the area of the intersections
```{r country and provinces areas}
# calculate the area of each intersection
intersections$intersection_area <- st_area(intersections)

# save info in a dataframe for easier retrival
intersections_df <- st_drop_geometry(intersections)

# add ID columns
intersections_df$intersection_ID <- paste(
  seq_len(nrow(intersections_df)),
  intersections_df$country,
  intersections_df$province ) %>% 
  str_replace_all(" ","_")

# place ID column as first one
intersections_df <- intersections_df[,c("intersection_ID","country","province","country_area","intersection_area")]
```

Note that due to small differences between the shapefiles of the countries and the provinces, the sum of the areas of the intersections falling within a given province is not exactly equal to that of the province itself.
```{r corrective factors for areas}
# compute the sum of the areas of the intersections
check_areas_intersections_df <- intersections_df %>%
  group_by(province) %>%
  summarize(sum_intersection_areas = sum (intersection_area)) %>% 
  left_join(provinces_areas_df)

# compare the sum of the areas of the intersections with the area of the province 
check_areas_intersections_df$area_correction_factor <- check_areas_intersections_df$sum_intersection_areas/check_areas_intersections_df$area
```

From now on, we use this total intersection area as our province area to avoid complications.
Compute the proportion of each province's total area (or, more precisely, of the sum of the intersections) fell in each intersection. 
```{r proportion of area by intersection}
# add sum of the areas of the intersections to the df of intersections
intersections_df <- left_join(
  intersections_df,
  check_areas_intersections_df[c("province","sum_intersection_areas")]
  )

# compute the proportion by dividing the areas
intersections_df$prop_intersection_area <- intersections_df$intersection_area/intersections_df$sum_intersection_areas
```

Now to estimate the rural population of each intersection, take the rural population of the province it belongs to and multiply by the proportion of the total area of the province (or more precisely of the sum of the intersections belonging to the province) that falls within the intersection.
```{r rural population of intersections}
# merge info on intersections and rural population of the provinces
intersections_pop_full_info_df <- left_join(
  intersections_df[c("intersection_ID","country","province","prop_intersection_area")],
  provinces_population_df[c("province","rural_pop_min","rural_pop_max")]) %>% setNames(c("intersection_ID","country","province","prop_intersection_area","province_rural_pop_min","province_rural_pop_max"))


# compute estimates of rural population of intersections
intersections_pop_full_info_df$intersection_rural_pop_min <- as.numeric(intersections_pop_full_info_df$province_rural_pop_min*intersections_pop_full_info_df$prop_intersection_area)
intersections_pop_full_info_df$intersection_rural_pop_max <- as.numeric(intersections_pop_full_info_df$province_rural_pop_max*intersections_pop_full_info_df$prop_intersection_area)
```

Now compute the urban population of each intersection starting from Hanson's dataset.
```{r urban population by intersection}
# assign each city to its closest intersection
closest_intersection_to_city_index <- st_nearest_feature(cities,intersections)
cities_in_intersections_df <- data.frame(
  Primary.Key = cities$Primary.Key,
  Ancient.Toponym = cities$Ancient.Toponym,
  pop_est = cities$pop_est,
  intersection_ID = intersections_df$intersection_ID[closest_intersection_to_city_index]
)

# compute the urban population by intersection
intersections_urban_population_df <- cities_in_intersections_df %>%
  group_by(intersection_ID) %>%
  summarize(intersection_urban_pop = sum(pop_est)) 
```

If there are intersections without cities, assign population 0.
```{r intersections without cities}
# add a row with value 0 
intersections_urban_population_df <- rbind(
  intersections_urban_population_df,
  data.frame(
    intersection_ID = intersections_df$intersection_ID[
      which(!intersections_df$intersection_ID %in%
              intersections_urban_population_df$intersection_ID)],
    intersection_urban_pop = 0
    )
)

# sort alphabetically
intersections_urban_population_df <- intersections_urban_population_df[order(intersections_urban_population_df$intersection_ID),]
```

Add this info to the df of intersections' populations
```{r urban, rural and total population of intersections}
# urban population
intersections_pop_full_info_df <- left_join(intersections_pop_full_info_df,
                                            intersections_urban_population_df)

# total population
intersections_pop_full_info_df$intersection_tot_pop_min <- 
intersections_pop_full_info_df$intersection_rural_pop_min + 
intersections_pop_full_info_df$intersection_urban_pop
intersections_pop_full_info_df$intersection_tot_pop_max <-
  intersections_pop_full_info_df$intersection_rural_pop_max + 
intersections_pop_full_info_df$intersection_urban_pop
```

Remark: the match is not perfect: there must be a city that is classifies as closer to Achaia than Asia once the intersections are taken. This should not be a huge problem in principle as the difference is small. But it is worth to take it under consideration.
```{r check that the total for the provinces almost matches}
intersections_pop_full_info_df %>% 
  group_by(province) %>%
  summarize(preliminary_sum_min=sum(intersection_tot_pop_max))  %>%
  left_join(
    provinces_macroregions_full_info_df[c("province","tot_pop_max","tot_pop_min")]
  )
```

Finally, group the intersections by country and save the population data
```{r group the intersections by country and save the population data}
# compute the population
countries_population_df <-  intersections_pop_full_info_df %>% 
  group_by(country) %>% 
  summarize(urban_pop= sum(intersection_urban_pop),
            rural_pop_min = sum(intersection_rural_pop_min),
            rural_pop_max = sum(intersection_rural_pop_max),
            tot_pop_min = sum(intersection_tot_pop_min),
            tot_pop_max = sum(intersection_tot_pop_max)) %>%
  as.data.frame() %>%
  na.omit()


rownames(countries_population_df) <- countries_population_df$country

# check validity of grand total
(sum(provinces_macroregions_full_info_df$tot_pop_min) == sum(countries_population_df$tot_pop_min) && sum(provinces_macroregions_full_info_df$tot_pop_max) == sum(countries_population_df$tot_pop_max)) 

# save results
#write.csv(countries_population_df,
#          "../../results/economic_complexity/countries_population.csv")
#write.csv(intersections_pop_full_info_df,
#          "../../results/economic_complexity/intersections_population_full_info.csv")
```

Then compute the ratios of rural population
```{r rural population ratios in countries}
countries_rural_pc_df <- data.frame(
  country = countries_population_df$country,
  "min" = countries_population_df$rural_pop_min/countries_population_df$tot_pop_min,
  "max" = countries_population_df$rural_pop_max/countries_population_df$tot_pop_max
) %>% na.omit()
```

#### Total number of person-years in each province
The above population estimates for the geographical units refer to peak population. We now use them to estimate how many person-years have been lived whilst the province was part of the Roman empire
```{r construct total population estimates for each province}
# read csv with info on when provinces were conquered/lost
province_dates_df <- read.csv("../../data/economic_complexity/province_dates.csv",sep=";")

# order alphabetically
provinces_population_df <- provinces_population_df[order(provinces_population_df$province),]
province_dates_df <- province_dates_df[order(province_dates_df$province),]

# build data.frame of population multipliers
n_iter <- 1000
years <- c(min(province_dates_df$start_date):-1,1:max(province_dates_df$end_date))
n_years <- abs(min(province_dates_df$start_date)) + max(province_dates_df$end_date)

# extract the growth rates from a triangular distribution 
growth_rate_samples <- rtri(n_iter,min=0,max=0.005,mode=0.0015)

# define the population functions
integrand_to_165AD <- function(x){(1-r)^(165-x)}
integrand_from_165AD <- function(x){(1-r)^(x-165)}

# initialise population results
provinces_total_population_df <- data.frame(
  iter = rep(seq_len(n_iter), each = nrow(provinces_population_df)),
  province = rep(provinces_population_df$province,n_iter),
  urb_pop_grand_total = 0,
  rural_pop_min_grand_total = 0,
  rural_pop_max_grand_total = 0,
  tot_pop_min_grand_total = 0,
  tot_pop_max_grand_total = 0)

for (i in seq_len(n_iter)) {
  r <- growth_rate_samples[i]
  for (j in seq_len(nrow(province_dates_df))){
    # find the start and end date of each province (THIS SHOULD BE DONE WITH VECTORS IN THE FUTURE)
    start_date <- province_dates_df$start_date[j]
    end_date <- province_dates_df$end_date[j]
    
    # find integral
    integral_to_165AD <- integrate(integrand_to_165AD,start_date,min(end_date,165))
    integral_from_165AD <- integrate(integrand_to_165AD,max(start_date,165),end_date)
    integral_total <- integral_to_165AD$value + integral_from_165AD$value

    # add values to df
    provinces_total_population_df[(i-1)*nrow(provinces_population_df)+j,
                                  c("urb_pop_grand_total",
                                    "rural_pop_min_grand_total",
                                    "rural_pop_max_grand_total",
                                    "tot_pop_min_grand_total",
                                    "tot_pop_max_grand_total")] <-
      integral_total*provinces_population_df[j,c("urban_pop","rural_pop_min","rural_pop_max","tot_pop_min","tot_pop_max")]
  }
  print(i)
}

# save results
#write.csv(provinces_total_population_df,"../../results/economic_complexity/provinces_total_population.csv")
```

#### Ratios population/epitaph
First, compute the ratios of the people per epitaph by provinces.
```{r population/epitaph by province}
# province
# count epitaphs by province
epitaphs_in_provinces_counts <- epitaphs_in_provinces$province %>% 
  table() %>%
  as.data.frame() %>%
  setNames(c("province","epitaphs"))

# initialise data frame of bias 
ratio_epitaphs_pop_provinces <- data.frame(
  provinces_population_df[,c("province","tot_pop_min","tot_pop_max")]) %>%
  left_join(epitaphs_in_provinces_counts)

# add columns of ratios between population estimates and epitaphs counts
ratio_epitaphs_pop_provinces$people_per_epitaph_min <- ratio_epitaphs_pop_provinces$tot_pop_min/ratio_epitaphs_pop_provinces$epitaphs
ratio_epitaphs_pop_provinces$people_per_epitaph_max <- ratio_epitaphs_pop_provinces$tot_pop_max/ratio_epitaphs_pop_provinces$epitaphs
```

Now, save the results
```{r save results}
# save results
#write.csv(ratio_epitaphs_pop_countries,
#          "../../results/economic_complexity/ratio_epitaphs_pop_countries.csv")
#write.csv(ratio_epitaphs_pop_provinces,
#          "../../results/economic_complexity/ratio_epitaphs_pop_provinces.csv")
```

Repeat the same for person-year by epitaph
First, compute the ratios of the people per epitaph by provinces.
```{r person_years/epitaph by province}
# province
# initialise data frame of bias 
ratio_epitaphs_tot_pop_provinces <- data.frame(
  provinces_total_population_df[,c("iter","province","tot_pop_min_grand_total","tot_pop_max_grand_total")]) %>%
  left_join(epitaphs_in_provinces_counts)

# add columns of ratios between population estimates and epitaphs counts
ratio_epitaphs_tot_pop_provinces$person_years_per_epitaph_min <- ratio_epitaphs_tot_pop_provinces$tot_pop_min_grand_total/ratio_epitaphs_tot_pop_provinces$epitaphs
ratio_epitaphs_tot_pop_provinces$person_years_per_epitaph_max <- ratio_epitaphs_tot_pop_provinces$tot_pop_max_grand_total/ratio_epitaphs_tot_pop_provinces$epitaphs
```

It is clear that the number of epitaphs does not correspond to the cumulative population.

# Table of occupations ---------------------------------------------------------
## Functions
The following functions receives as input a set of inscriptions with occupations, assigned to geographical units, and outputs the occupation tables by geographical units.
```{r function for occupation tables}
table_of_occupations <- function(insc_w_occ){
  
  # ensure that the info on geographical unit (gu) is included in the input
  gu_type <- which_gu(insc_w_occ)

  # create list of geographical units
  if (gu_type == "province") {
    gu <- provinces$province
  } else if (gu_type == "country") {
    # note that some disputed areas in the intersections are not assigned to any country, hence exclude na
    gu <- na.omit(unique(intersections$country))
  } # no need to check for other cases as which_gu already takes care of it
  
  # unnest the list of occupations in each inscription (based on the standard occupational terms)
  insc_w_occ_unnested <- unnest(insc_w_occ,"occups_std")
  
  # initialise table
  occ_table <- table(insc_w_occ_unnested[[gu_type]],
                     insc_w_occ_unnested$occups_std)

  # add zero rows for gus without inscriptions
  gu_not_in_occ_table <- gu[!gu%in%insc_w_occ_unnested[[gu_type]]]
  occ_table_zeros <- matrix(0, nrow= length(gu_not_in_occ_table), ncol = ncol(occ_table))
  rownames(occ_table_zeros) <- gu_not_in_occ_table
  occ_table <- rbind(occ_table,occ_table_zeros) %>% as.table()

  # sort the table based on row and column sums
  occ_table <- sort_table(occ_table)
  
  # return occupation table
  return(occ_table)
} 
```

This function takes as input an occupational table and a geographical unit and plots the occupation table as a heatmap
```{r function for occupational tables as heatmaps}
occ_heat_map <- function(occ_table){
  # retrieve gu type
  gu <- which_gu(occ_table)
  
  # Convert the table to a dataframe
  occ_table_heatmap <- as.data.frame(occ_table) %>% setNames(c(gu,"occupation","frequency"))
  occ_table_heatmap$log_frequency <- log(occ_table_heatmap$frequency)

  # prepare the plot
  occ_table_heatmap_plot <- ggplot(occ_table_heatmap, aes(occupation, !!sym(gu), fill=log_frequency)) + 
    geom_tile() +
    scale_fill_gradient(low = "white", high = "red") +
    labs(x = "occupation", 
        y = gu, 
        fill ="log(frequency)",
        title = paste0("Frequency of occupations in inscriptions by ",gu)) +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1,size = 2))
}
```

## Compute occupational tables 
We now compute the occupational tables and heat maps for countries and provinces
```{r compute occupational tables}
province_table <- table_of_occupations(inscriptions_w_occupations_in_provinces)
country_table <- table_of_occupations(inscriptions_w_occupations_in_countries)
```

Plot and save heat maps
```{r plot and save heatmaps}
# define heatmaps
province_table_heatmap <- occ_heat_map(province_table)
country_table_heatmap <- occ_heat_map(country_table)

# plot and save
plot_and_save(province_table_heatmap,width = 15, height = 10)
plot_and_save(country_table_heatmap,width = 15, height = 10)
```

# Uncertainty and bias mitigation ---------------------------------------------- 
## Uncertainty intro
The above tables were constructed by using the occupation data as it was directly extracted from the inscriptions. 

However, inscriptions do not represent a realistic view of the real occupational structure. Sources of bias include:
* Gender bias: inscriptions in general contain information about men than women. 
* Research bias: uneven research interest in different occupations and geographical areas, imperfect detection method, etc. 
* Occupation bias: some occupations are systematically underreported (e.g., farmers)
* Language bias: the dataset includes only latin inscriptions, but a large number of inscriptions were written in Greek, especially in the Eastern side of the Roman empire. 

In this section we propose to systematically address these biases with the aim to reach more accurate estimates of the real distribution of occupations.

We do this by constructing functions to each source of bias.

## Functions to address bias
### Gender bias
We multiply women-only occupations (which for simplicity we take to be those with female-only names), by a corrective factor, derived from the men/women ratio in funerary inscriptions. 
```{r extract info about gender inscriptions}
# compute a table of frequency of gender from funerary inscriptions
EDH_epitaphs <- EDH_people[EDH_people$type_of_inscription_clean=="epitaph",]
EDH_gender <- table(EDH_epitaphs$gender)
EDH_gender_NA <- sum(is.na(EDH_epitaphs$gender))
print(EDH_gender)
print(paste("NA values:", EDH_gender_NA))

# select female occupations
occupations_female <- occupations_all$std_spelling[occupations_all$gender=="female"]
```

Gender is specified as "female", "male", but also uncertain but probably male "M?", uncertain but probably female "W?", and undefined (NA). We account for this in the definition of the ratio.
```{r compute the men/women ratio in funerary inscriptions}
gender_ratio_simple <- as.numeric(EDH_gender["male"]/EDH_gender["female"])
gender_ratio_max <- (EDH_gender[["male"]]+EDH_gender[["M?"]]+EDH_gender[["W?"]]+EDH_gender_NA)/EDH_gender[["female"]]
gender_ratio_min <- EDH_gender[["male"]]/(EDH_gender[["female"]]+EDH_gender[["M?"]]+EDH_gender[["W?"]]+EDH_gender_NA)
```

Randomly generate bias correction values based on a triangular distribution with range (gender_ratio_min,gender_ratio_max) and peak at gender_ratio_simple.
```{r vector of gender ratio bias}
# randomly generate the vector according to a triangular distribution
gender_ratio_v <- rtri(n = 1000,
                       min = gender_ratio_min,
                       max = gender_ratio_max,
                       mode = gender_ratio_simple)

# visual check of the distribution of the results
plot(hist(gender_ratio_v,20))
```

Define a function to correct gender bias of a given occupational table
```{r gender bias function}
gender_bias <- function(table, bias){
  # update columns with female occupations
  table[,colnames(table)%in%occupations_female] <- bias*table[,colnames(table)%in%occupations_female] 
  return(table)
}
```

### Agriculture bias
#### Intro
The Roman Empire was primarily a rural society.
Our above estimates indicate that the rural population would have comprised roughly between 84% and 86% of the total population, though this varies widely among regions.
Most of these people would have been involved in agriculture, but evidence of farming as an occupation in inscriptions is extremely scarce (only 7 references in the whole empire).

We conclude our attempt to reduce bias in the tables by inflating the count of the generic profession of farmer (agricola).
We assume for simplicity that generic agricolae and other food-production-related occupations would involve a proportion of the total occupations roughly similar to that of the rural population.
(Rk: in practice there would be a certain proportion of rural population not involved in agriculture and a certain proportion of urban population involved in it. For now we make the simplifying assumption that these would roughly cancel each other off).

Let "a" be the number of "agricola"
Let "f" be the count of people involved in other "food-production" activities (based on Harris category).
Let "d" be the sum of all occupations different from food production.
Let "r" be the proportion rural population in a geographical unit.
Let "p" be a parameter, with p very close to 1.

Then the above assumption translates into the following simple equation: a+f = pr(a+f+d),
which yields a = dpr/(1-pr)-f.
RK: when p=1, 1-pr=u, where u is the urban population, so a=dr/u-f

Note that since r is itself uncertain within a range (r_min,r_max), we define a further parameter "s" between 0 and 1 and interpolate between the min and max values. 
The proportion r of rural population is then calculated as: 
r = (1-s)r_min+(s)r_max

#### Implementation
Implement the above formula as an algorithm
```{r function to expand the count of "agricola" in the table}
all_food_prod_occs <- occupations_status$std_spelling[occupations_status$Harris_Category=="Food-Production"]
other_food_prof_occ <- all_food_prod_occs[!all_food_prod_occs=="agricola"]
non_food_prod_occ <- occupations_status$std_spelling[!occupations_status$std_spelling %in% all_food_prod_occs]

# ensure parameter p is always slightly below 1 (otherwise divide by inf, but it makes sense logically)
agricola_correction <- function(occupation_table, p, s){
  # retrieve gu type
  gu_type <- which_gu(occupation_table)
  
  # find names of geographical units
  gu <- rownames(occupation_table)
  
  # initialise the output
  output_table <- occupation_table
  
  # define the equation variables
  f <- rowSums(occupation_table[,other_food_prof_occ])
  d <- rowSums(occupation_table[,non_food_prod_occ])
  
  # info on population
  if (gu_type == "province") {
    pop <- provinces_rural_pc_df
  } else if (gu_type == "country") {
    # note that a few hundred people are not associated with any country, hence the na.omit is necessary
    pop <- na.omit(countries_rural_pc_df)
  } else {
    stop("please input a valid gu_type")
  }
  rownames(pop) <- pop[[gu_type]]
  # sort as in the occupation table
  pop <- pop[gu,]
  
  # retrieve the proportion of rural population
  r_min <- pop[gu,"min"]
  r_max <- pop[gu,"max"]
  r <- (1-s)*r_min+s*r_max
  
  # compute number of agricola
  a <- apply(cbind(d*p*r/(1-p*r)-f,occupation_table[,"agricola"]),1,max) # to ensure that we do not go below what is recorded

  # update table
  output_table[,"agricola"] <- a

  # sort
  output_table <- sort_table(output_table)
  
  return(output_table)
}
```

### Social status bias
#### Intro
Roughly speaking, higher status people tend to be over-represented compared to lower status people in inscriptions.
As certain occupations were primarily performed by people of high or low status, the disparity in representation affects occupation counts.
In this section, we propose a simple (and improvable) mechanism to address this bias.

Among all inscriptions with occupations, we select those included in EDH and that thus contain references to people. 
Among these, we focus on those that contain references to the people's status. 
We distinguish people into two broad categories: elites and non-elite people, 
where by elites we mean people whose status is included in c("equestrian order", "decurial order, higher local offices", "senatorial order", "equestrian order", "military personnel", "lower local offices, administration of imperial estates", "emperor / imperial household", "rulers (foreign)"), and by non-elites we mean people whose status in included in c("freedmen / freedwomen", "slaves", "Augustales").
Among inscriptions which contain references to occupations,  we find 7358 references to elite people,	1258 references to non-elites, thus yielding a ratio of approximately 5.85 elite people per non-elite person, which is highly unrealistic. 
Note however that most inscriptions with occupations contain no reference to status.

Each occupation is classified in one of nineteen Harris Categories.
For each Harris Category, we count the co-occurrence of references to elite/non-elite statuses with occupations in that category.
(Rk: At the moment we are only looking at co-occurrence, and thus we do not have a mechanism to check whether the person whose status is mentioned is also the person who performs a given occupation. This can lead to wrong associations, e.g., "Senator X hires a cook" would associate senator and cook, and thus necessitates a more careful assessment of the source material in the future.)
We then compute how commonly elite/non-elite people were employed in a given category (e.g., 10% of non-elite people were employed in clothing-related occupations, but only 1% of elite people were). 
This allows us to determine whether a category was mostly an elite or non-elite one.

We use this information to inflate the counts of occupations in categories dominated by non-elites by a fixed factor.
Scheidel and Friesen (2007) proposed that the Ã©lite population (including the military) constituted about 3% of the population. 
If we exclude the farmers (who almost do not appear in the inscription data), we have between 13.6 and 16.2 percent of the population left (see agricola correction). 
Hence the expected ratio of non-elites to elites is between 3.5 and 4.4, which requires a correction factor between 20.475 and 25.74. 
To allow for uncertainty in the estimates, we select our corrective factor in the range 15 to 30, with a mean of 22.5.

Rk: These factors are arbitrary because we do no know exactly how many people were elite or non-elite in Ancient Rome.
A more careful approach would involve assuming simplistically that if one had status, then they would include it in the inscriptions, and thus assuming that those who did not indicate their status were probably non-elites. This would be roughly 3/4 of the total.

#### Implementation
Classify occupations as elite or non-elite based on their Harris category
```{r status and occupations}
# assign status to each Harris category
Harris_category_status <- data.frame(
  Harris_Category = sort(unique(occupations_all$Harris_Category)),
  elite = c(TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE)) #Rk: it contain "Unclassfied" (sic) as category

# for each occupation, classify it as either elite or non elite
occupations_status <- occupations_all[,c("std_spelling","Harris_Category")] %>% 
  unique() %>%
  left_join(Harris_category_status)

# select only occupations actually in the data
occupations_status <- occupations_status[occupations_status$std_spelling %in% col_names_c,]
non_elite_occupations <- occupations_status$std_spelling[!occupations_status$elite]
```

Define a function to inflate the non-elite occupation counts by a corrective factor
```{r function to inflate a table by a given value}
status_bias_correction <- function(occupation_table, corrective_factor){
  output_table <- occupation_table
  output_table[,non_elite_occupations] <-  output_table[,non_elite_occupations] *corrective_factor
  return(output_table)
}
```

Sample the corrective factor from a certain interval and output several estimates of corrections
```{r sample the corrective factor output several corrections}
status_bias_correction_stochastic <- function(occupation_table, 
                                              cf_min, 
                                              cf_max,  
                                              cf_mode=(cf_min+cf_max)/2, 
                                              n_samples){
  # initialise empty list
  output_table_list <- list()
  
  # set for reproducibility
  set.seed(123)
  
  # check that input values are valid
  if (cf_min>cf_mode|cf_min>cf_max|cf_mode>cf_max){
    stop("Provice valid values for the corrective factors cf_min, cf_max, and cf_mode (if specified), so that cf_min<=cf_mode<=cf_max")
  }
  
  # sample corrective factors from triangular distribution
  corrective_factors <- rtri(n = n_samples,
                       min = cf_min,
                       max = cf_max,
                       mode = cf_mode)
  
  # fill list with each corrective factor
  for (i in seq_len(n_samples)){
    # adjust
    output_table_i <- status_bias_correction(occupation_table,corrective_factors[i])
    # sort
    output_table_i <- output_table_i %>% sort_table()
    # store
    output_table_list[[i]] <- output_table_i
  }
  
  return(output_table_list)
}
```

### Language 
#### Intro
Many inscriptions found within the limits of the Roman Empire were written in Greek. These were primarily concentrated in the Eastern Roman Empire. We need to account for them to avoid language bias.

In this section we use the counts of greek inscriptions by geographical units (gu) that were computed in the previous sections.
We assume the same distribution of professions in the (not analysed) Greek inscriptions and use their locations to proportionally adjust the occupation tables in the geographical units. 

#### Implementation
Only a small proportion of all Greek inscriptions were likely occupation-related.
We compute the proportion of occupation-related inscriptions among all Latin inscriptions and assume the same proportion holds for Greek inscriptions to adjust the inscriptions counts.

First, compute the proportion of occupation-related Latin inscriptions and the number of occupations in each occupation-related inscription
```{r analyse occupation-related Latin inscriptions}
# find the proportion of occupation-related Latin inscriptions of 
proportion_occupation_related_Latin_inscriptions <- nrow(inscriptions_w_occupations)/nrow(inscriptions)

# retrieve the distribution of the number of occupations per occupation-related Latin inscription 
n_occupations_per_inscription_Latin <- inscriptions_w_occupations$occups_N

# the distribution is heavily left-skewed with a median and mean of 1 and ~1.24 respectively
hist(n_occupations_per_inscription_Latin, 100)
```

The following function estimates the number of occupation-related Greek inscription by geographical unit
```{r estimate the number of occupation-related greek inscriptions per geographical unit}
# define function
occupation_related_greek_inscriptions_in_gu_f <- function(greek_insc_in_gu){
  
  # create a table of counts of greek-inscription by geographical unit (country or province).
  gu <- which_gu(greek_insc_in_gu)
  greek_insc_in_gu_counts <- table(greek_insc_in_gu[[gu]])
  
  # estimate the number of occupation-related greek inscriptions by gu by assuming the same proportion as the Latin case
  greek_insc_w_occupations_in_gu_counts_df <- as.data.frame(greek_insc_in_gu_counts*proportion_occupation_related_Latin_inscriptions) %>% 
    setNames(c(gu,"frequency"))
  
  # We round these values
  greek_insc_w_occupations_in_gu_counts_df$frequency_rounded <- round(greek_insc_w_occupations_in_gu_counts_df$frequency) 
  
  # output the table
  return(greek_insc_w_occupations_in_gu_counts_df)
}

# apply to countries and provinces
greek_inscriptions_w_occupations_in_countries_counts_df <- occupation_related_greek_inscriptions_in_gu_f(greek_inscriptions_in_countries)
greek_inscriptions_w_occupations_in_provinces_counts_df <- occupation_related_greek_inscriptions_in_gu_f(greek_inscriptions_in_provinces)
```

We estimate the number of occupations in each occupation-related Greek inscription by sampling from the distribution of the number of occupations in occupation-related Latin inscriptions.
```{r estimated the number of occupations in each occupation-related greek inscription}
# function
occupations_in_occupation_related_greek_inscriptions_in_gu_f <- function(greek_insc_w_occupations_in_gu_counts_df, n_samples=1000){
  # set seed for reproducibility
  set.seed(123)
  
  # we sample n_samples=1000 times (the default).
  sampled_n_occupations_per_greek_inscriptions_in_gu <- replicate(
    n_samples,
    sample(n_occupations_per_inscription_Latin,
           size = round(sum(greek_insc_w_occupations_in_gu_counts_df$frequency_rounded)),
           replace = TRUE))
  
  # return output
  return(sampled_n_occupations_per_greek_inscriptions_in_gu)
}

# apply to countries and provinces
sampled_n_occupations_per_greek_inscriptions_in_countries <- occupations_in_occupation_related_greek_inscriptions_in_gu_f(greek_inscriptions_w_occupations_in_countries_counts_df)
sampled_n_occupations_per_greek_inscriptions_in_provinces <- occupations_in_occupation_related_greek_inscriptions_in_gu_f(greek_inscriptions_w_occupations_in_provinces_counts_df)
```

Note that the number of occupation per occupation-related Greek inscription is approximately normally distributed around the mean of the number of occupations per occupation-related Latin inscription (as expected)
```{r plot distribution of occupations per occupation-related inscription}
plot_occupation_distribution <- function(sampled_data, latin_data, unit_type = "country") {
  
  # Check if unit_type is valid
  if (!unit_type %in% c("country", "province")) {
    stop("unit_type must be either 'country' or 'province'")
  }
  
  # Create the plot title based on unit_type
  plot_title <- paste("Distribution of Average Occupations per Greek Inscription by", unit_type)
  
  # Plot histogram
  hist(colMeans(sampled_data), 
       breaks = 100,
       main = plot_title,
       xlab = "Average Number of Occupations per Greek Inscription",
       ylab = "Frequency",
       col = "lightgray")
  
  # Add vertical lines for the means
  abline(v = mean(latin_data), col = "red", lwd = 2)
  abline(v = mean(sampled_data), col = "blue", lwd = 2)
  
  # Add a legend with labels based on unit_type
  legend("topright", 
         legend = c(paste("Mean Occupations per Latin Inscription (", unit_type, ")", sep = ""),
                    paste("Mean Occupations per Greek Inscription (Sampled, ", unit_type, ")", sep = "")),
         col = c("red", "blue"), 
         lwd = 2)
}

# Countries
plot_occupation_distribution(
  sampled_data = sampled_n_occupations_per_greek_inscriptions_in_countries,
  latin_data = n_occupations_per_inscription_Latin,
  unit_type = "country"
)

# Provinces
plot_occupation_distribution(
  sampled_data = sampled_n_occupations_per_greek_inscriptions_in_provinces,
  latin_data = n_occupations_per_inscription_Latin,
  unit_type = "province"
)
```

Say that a geographical unit "g_i" contains "n_i" occupation-related inscriptions. 
By summing the first n_1 values of a column of the above sample, we obtain the total number of occupations in g_1. 
By summing the n_2 values between n_1+1 and n_1+n_2 we obtain the total number of occupations in g_2. etc.
We thus obtain counts of occupations in each geographical unit.
```{r total occupations by gu from Greek inscriptions}
# function
occupations_in_greek_inscriptions_in_gu_counts_f <- function(
    sampled_n_occupations_per_greek_inscriptions_in_gu,
    greek_inscriptions_w_occupations_in_gu_counts_df,
    n_samples=1000){
  
  # detect which geographical unit we are using
  gu <- which_gu(greek_inscriptions_w_occupations_in_gu_counts_df)

  # avoid zeros for simplicity
  greek_inscriptions_w_occupations_in_gu_counts_no_zeros_df <-   greek_inscriptions_w_occupations_in_gu_counts_df[!greek_inscriptions_w_occupations_in_gu_counts_df$frequency_rounded==0,]
  
  # Initialize an empty matrix to store the results
  occupations_in_greek_inscriptions_in_gu_counts <- matrix(
    nrow = nrow(greek_inscriptions_w_occupations_in_gu_counts_no_zeros_df), 
    ncol = ncol(sampled_n_occupations_per_greek_inscriptions_in_gu))
  
  # Loop through each pair of start and end indices
  end_idx <- 0
  start_idx <- 1
  
  for (i in seq_len(nrow(greek_inscriptions_w_occupations_in_gu_counts_no_zeros_df))) {
    # define end_index
    end_idx <- end_idx + 
      greek_inscriptions_w_occupations_in_gu_counts_no_zeros_df$frequency_rounded[i]
    
    # Calculate the column sums for the specified rows
    if (end_idx > start_idx){
      occupations_in_greek_inscriptions_in_gu_counts[i, ] <-
        colSums(sampled_n_occupations_per_greek_inscriptions_in_gu[start_idx:end_idx, ])
    } else {
      occupations_in_greek_inscriptions_in_gu_counts[i, ]<-
        sampled_n_occupations_per_greek_inscriptions_in_gu[start_idx,]
    }
  
    # redefine start-index for next iteration
    start_idx <- end_idx +1
  }
  
  # transform in dataframe
  occupations_in_greek_inscriptions_in_gu_counts_df <- data.frame( 
    gu = greek_inscriptions_w_occupations_in_gu_counts_no_zeros_df[[gu]],
    occupations_in_greek_inscriptions_in_gu_counts)
   
  # give meaningful names to rows and columns
  colnames(occupations_in_greek_inscriptions_in_gu_counts_df) <- c(gu,paste0("it_",seq_len(n_samples)))
  rownames(occupations_in_greek_inscriptions_in_gu_counts_df) <-
    occupations_in_greek_inscriptions_in_gu_counts_df[[gu]]

  return(occupations_in_greek_inscriptions_in_gu_counts_df)
}

# countries
occupations_in_greek_inscriptions_in_countries_counts_df <- occupations_in_greek_inscriptions_in_gu_counts_f(
  sampled_n_occupations_per_greek_inscriptions_in_countries,
  greek_inscriptions_w_occupations_in_countries_counts_df
)
# provinces
occupations_in_greek_inscriptions_in_provinces_counts_df <- occupations_in_greek_inscriptions_in_gu_counts_f(
  sampled_n_occupations_per_greek_inscriptions_in_provinces,
  greek_inscriptions_w_occupations_in_provinces_counts_df
)
```
 
For each geographical unit, we have derived an occupational distribution based on Latin inscriptions. 
Now, we want to proportionally redistribute Greek occupation counts across specific occupations within each gu.

However, using the distribution of Latin inscriptions we risk assigning a large number of Greek inscriptions to only a few occupation types in regions where Greek inscriptions are prevalent but Latin inscriptions are scarce.
As observed in the previous maps, this occurs in the Eastern Roman Empire, where Greek inscriptions are concentrated but Latin ones are comparatively rare. 
To address this imbalance, we will not only draw from the occupational distribution of each specific geographical unit but will also incorporate data from nearby units and from the Empire as a whole.

To define the surroundings of a geographical unit, we use a threshold r, including all geographical units located at least partially within this distance from the focal unit.

We consider three different scenarios, each based on distinct assumptions:
0. GU only: Only the inscriptions in the each gu are considered (corresponding to r = 0km)
1. Local Influence Dominates: Local environment and culture strongly influence occupational structure, so we draw exclusively from nearby provinces (r = 100km)
2. Wide Regional Influence: Wider regional occupational structure impacts the local structure (r = 1000km)
3. No Proximity Influence: Proximity is irrelevant, so we rely on the Empire-wide occupational distribution (r >= diameter of the Empire)

Note that studies in economic complexity (e.g. Hidalgo) indicate that proximity influences occupational structures, making Scenario 3 a null model for comparison.

Find surroundings of a geographical unit.
```{r surroundings of provinces and countries}
compute_surroundings <- function(geographical_units, unit_type, max_distance_meters) {
  
  # Ensure max_distance is in meters and in correct units
  max_distance <- set_units(max_distance_meters, "m")
  
  # Extract the relevant name and geometry columns based on the unit type
  if (unit_type == "province") {
    unit_names <- geographical_units$province
  } else if (unit_type == "country") {
    # Handle special cases for split countries
    # "PSE" (Palestine) appears twice (one territory corresponds to the West Bank and the other to the Gaza Strip)
    geographical_units <- geographical_units[ , c("iso3", "geometry")]
    geographical_units$geometry[geographical_units$iso3 == "PSE"] <- 
      st_union(na.omit(geographical_units[geographical_units$iso3 == "PSE", ]))
    geographical_units <- unique(geographical_units)
    unit_names <- geographical_units$iso3
  } else {
    stop("Invalid unit_type. Choose either 'province' or 'country'.")
  }
  
  # Initialise a list to store surrounding units for each geographical unit
  surrounding_list <- list()
  
  # Deal with the special cases: if distance is 0, include only each gu, if distance is very large, include all the gus
  if (max_distance_meters == 0){
     for (unit in unit_names) {
       surrounding_list[[unit]] <- unit
     }
  } else if (max_distance_meters > 4999999){
     for (unit in unit_names) {
       surrounding_list[[unit]] <- unit_names
     }
  } else {
    # Compute the distance matrix
    distance_matrix <- st_distance(geographical_units)
    rownames(distance_matrix) <- unit_names
    colnames(distance_matrix) <- unit_names

    # Populate the surrounding list based on max distance
    for (unit in unit_names) {
      surrounding_list[[unit]] <- unit_names[which(distance_matrix[unit, ] < max_distance)]
      }
  }
  
  return(surrounding_list)
}

# select countries
selected_countries <- world_sf[world_sf$iso3 %in% occupations_in_greek_inscriptions_in_countries_counts_df$country,] 

# initialise list of lists of surrounding geographical units (depending on the threshold)
surrounding_countries_list <- list()
surrounding_provinces_list <- list()

# define a vector of distance thresholds
max_distances <- c(0, 0.001, 100,1000, 5000)

# for each threshold, compute the surrounding countries
for(m in max_distances){
  #  countries
  surrounding_countries_list[[paste0("r_",m)]] <- compute_surroundings(
    selected_countries, 
    unit_type = "country", 
    max_distance_meters = m*1000)
  
  # provinces
  surrounding_provinces_list[[paste0("r_",m)]] <- compute_surroundings(
    provinces, 
    unit_type = "province", 
    max_distance_meters = m*1000)
}

```

We now define the four distributions of occupations as stated above. Note that there is no need to normalise them as we will sample from them.
```{r occupation tables for provinces and surroundings}
# define function to compute occupation tables of gu and surroundings
compute_surroundings_tables <- function(gu_table,gu_surroundings){
  # initialise list of tables
  gu_and_surroundings_table <- gu_table
  
  for (gu in rownames(gu_table)) {
    # compute the occupations in the gu and surroundings
    gu_and_surroundings_occupations <- gu_table[rownames(gu_table) %in% gu_surroundings[[gu]],]
    
    # if the neighbourhood contains only the province itself, return it, otherwise use colSums
    if (is.matrix(gu_and_surroundings_occupations)){
      gu_and_surroundings_occupations <- colSums(gu_and_surroundings_occupations)
    }
  
  # store results
  gu_and_surroundings_table[gu,] <- gu_and_surroundings_occupations
  }
  
  return(gu_and_surroundings_table)
}

# compute them for various maximum distance thresholds
province_and_surroundings_tables <- list()
country_and_surroundings_tables <- list()

for (m in max_distances) {
  # provinces
  surrounding_provinces <-  surrounding_provinces_list[[paste0("r_",m)]]
  province_and_surroundings_tables[[paste0("r_",m)]] <- compute_surroundings_tables(province_table,surrounding_provinces)
  
  # countries
  surrounding_countries <-  surrounding_countries_list[[paste0("r_",m)]]
  country_and_surroundings_tables[[paste0("r_",m)]] <- compute_surroundings_tables(country_table,surrounding_countries)
}
```

Now that we have the distributions, we want to sample with replacement from these sets with sample size equal to the estimated number of occupation-related Greek inscriptions. This will give us estimated distributions of occupations deriving from the Greek inscriptions
```{r sample occupations from provinces and surroundings}
greek_sample_f <- function(gu_table,
                           occupations_in_greek_inscriptions_in_gu_counts_df,
                           gu_and_surroundings_table){
  
  # for reproducibility
  set.seed(1)
  
  # retrieve number of samples
  n_samples <- ncol(occupations_in_greek_inscriptions_in_gu_counts_df)-1
  
  # retrieve gu_type
  gu_type <- which_gu(occupations_in_greek_inscriptions_in_gu_counts_df)
  
  #initialise the list
  gu_table_greek_list <- list()
  
  # loop over all iterations of occupations in greek inscriptions
  for (i in seq_len(n_iterations)) {
  
    #initialise table
    gu_table_greek <- gu_table
    gu_table_greek[,] <- 0
    
    # build
    for (gu in rownames(gu_table)) {
      if (gu %in% occupations_in_greek_inscriptions_in_gu_counts_df[[gu_type]]){
        # extract the info for a gu
        gu_counts <- gu_and_surroundings_table[gu,]
        
        # Check that not all values are zero
        if (sum(gu_counts) > 0){
           # convert into a distribution of occupations. Each occupation appears as many times as the province 
        gu_distribution <- rep(names(gu_counts),gu_counts)
      
        # sample
        occupations_sample <- sample(
          gu_distribution,
          size = occupations_in_greek_inscriptions_in_gu_counts_df[gu,i+1],
          replace = TRUE
        )
      
        # as table
        occupations_sample_table <- table(occupations_sample)
      
        # add to the table
        gu_table_greek[gu,names(occupations_sample_table)] <- occupations_sample_table
          
        }
      }
    }
  
    # add to the list
    gu_table_greek_list[[paste0("it_",i)]] <- gu_table_greek
    
    # check progression
    if (i %%100 ==0){
      print(paste(i,"samples out of",n_samples,"completed."))
    }
  }
  
  # return table of occupations in greek inscriptions
  return(gu_table_greek_list)
}
```

Apply the above function to countries and provinces and obtain possible occupation tables for the greek provinces
```{r sample Greek occupations in provinces and countries}
#initialise the lists
province_table_greek_lists <- list()
country_table_greek_lists <- list()

for (m in max_distances) {
  # province
  province_table_greek_lists[[paste0("r_",m)]] <- greek_sample_f(
    province_table,
    occupations_in_greek_inscriptions_in_provinces_counts_df,
    province_and_surroundings_tables[[paste0("r_",m)]])
  
  # country
  country_table_greek_lists[[paste0("r_",m)]] <- greek_sample_f(
    country_table,
    occupations_in_greek_inscriptions_in_countries_counts_df,
    country_and_surroundings_tables[[paste0("r_",m)]])
  
  # check progress
  print(m)
}

# save the data
saveRDS(province_table_greek_lists, file = "../../results/economic_complexity/province_table_greek_lists.rds")
saveRDS(country_table_greek_lists, file = "../../results/economic_complexity/country_table_greek_lists.rds")
```

Finally we add the tables of greek and latin inscriptions together to obtain the result 
```{r add the tables}
greek_latin_tables_f <- function(gu_table,gu_table_greek_lists){
  
  # initialise list of results for all thresholds
  gu_table_greek_latin_lists <- list()
  
  for (m in max_distances) {
    # initialise list for threshold m
    gu_table_greek_latin_list_m <- list()
    
    # retrieve greek list and number of samples
    gu_table_greek_lists_m <- gu_table_greek_lists[[paste0("r_",m)]] 
    n_samples <- length(gu_table_greek_lists_m)
    
    # compute a table for each sample
    for(i in seq_len(n_samples)){
      # sum
      gu_table_greek_latin <- gu_table_greek_lists_m[[paste0("it_",i)]] + gu_table
      
      # sort 
      gu_table_greek_latin <- sort_table(gu_table_greek_latin)
     
      # add to list
      gu_table_greek_latin_list_m[[paste0("it_",i)]] <- gu_table_greek_latin 
    }
    
    # add list for threshold m to result
    gu_table_greek_latin_lists[[paste0("r_",m)]] <- gu_table_greek_latin_list_m
  }
  
  # return list of occupation estimates from both Latin and Greek inscriptions
  return(gu_table_greek_latin_lists)
}

# apply to provinces and country
province_table_greek_latin_lists <- greek_latin_tables_f(province_table,province_table_greek_lists)
country_table_greek_latin_lists <- greek_latin_tables_f(country_table,country_table_greek_lists)

# save results
saveRDS(province_table_greek_latin_lists, file = "../../results/economic_complexity/province_table_greek_latin_lists.rds")
saveRDS(country_table_greek_latin_lists, file = "../../results/economic_complexity/country_table_greek_latin_lists.rds")
```

We output examples of the heat map for each choice of threshold m
```{r greek latin inscriptions example heat map for each choice of threshold m}
for (m in max_distances) {
  # select examples for each threshold
  province_table_greek_latin_example <- province_table_greek_latin_lists[[paste0("r_",m)]][["it_1"]]
  country_table_greek_latin_example <- country_table_greek_latin_lists[[paste0("r_",m)]][["it_1"]]

  # create heat map
  province_table_greek_latin_heatmap_m <- occ_heat_map(province_table_greek_latin_example)
  country_table_greek_latin_heatmap_m <- occ_heat_map(country_table_greek_latin_example)

  # save it
  plot_and_save(province_table_greek_latin_heatmap_m,title=paste0("province_table_greek_latin_heatmap_m_",m),width=10)
  plot_and_save(country_table_greek_latin_heatmap_m,title=paste0("country_table_greek_latin_heatmap_m_",m),width=10)
}
```

Generally speaking, larger thresholds (i.e., more diverse sampling) yield more triangular matrices.

### Research bias
#### Intro
We are interested in reconstructing the occupational structure in each geographical unit at a given point in time.
Our inscription-based datasets offer a cumulative view of occupations over the entire duration of the Empire.
If the rate of inscription production were constant across individuals, we would expect the total number of inscriptions in a region to be proportional to the total number of person-years lived in that region while it was under Roman rule.

The final step is to recalibrate the total number of occupations across all provinces to reflect the population in each geographical unit at a given point in time.
This adjustment helps to account for potential differences in the rate of inscription production across various geographical units and corrects for biases arising from the uneven extent of archaeological excavations in different regions of the Roman Empire.
#### Implementation
We define a function that returns an occupation table with rowsums proportional to the population of a province
```{r research correction}
research_correction <- function(occupation_table,p){
  # check validity of parameter p
  if (p<0|p>1){
    stop("p must be between 0 and 1")
  }
  
  # retrieve gu type
  gu_type <- which_gu(occupation_table)
  
  # find names of geographical units (in the same order as they appear in the occupation table)
  gu <- rownames(occupation_table)
  
  # select appropriate population table
  if (gu_type == "province"){
    gu_pop_df <-  provinces_population_df
  } else if (gu_type == "country"){
    gu_pop_df <-  na.omit(countries_population_df)
  } # note that if it is neither, the gu_type function will have already stopped the process 
  
  # compute corrective factor ((1-p)*pop_min + p*pop_max), with 0<=p<=1
  gu_pop_correction_df <- (1-p)*gu_pop_df[gu,"tot_pop_min"]+p*gu_pop_df[gu,"tot_pop_max"]
  
  # initialise the output
  output_table <- occupation_table/pmax(rowSums(occupation_table),1)*gu_pop_correction_df # pmax to avoid dividing by 0
  
  # sort
  output_table <- sort_table(output_table) 
  
  #output
  return(output_table)
}
```

## Apply bias correction 
### Bias correction for the province table.
Order to follow:
1. Language
2. Status bias
3. Gender
4. Agricola
5. Research bias NEW

#### Simple Example
```{r apply corrections}
# original table 
start_table <- province_table

# language bias
table_step_1 <- province_table_greek_latin_lists$r_500[[1]]

# delete rows which sum to zero (for computation of complexity) fk 
start_table_no_zeros <- start_table[rowSums(start_table)>0,]
table_1_no_zeros <- table_step_1[rowSums(table_step_1)>0,]

# status bias
table_step_2 <- status_bias_correction_stochastic(table_1_no_zeros,15,30,n_samples=1)[[1]]

# gender bias
table_step_3 <- gender_bias(table_step_2,gender_ratio_v[1])

# agricola bias
table_step_4_min <- agricola_correction(table_step_3,0.95,0)

# research bias
table_step_5_min <- research_correction(table_step_4_min,0)
```

Define and plot the heatmaps at each stage
```{r Heat maps of example}
# compute heatmaps
start_table_no_zeros_hm <- occ_heat_map(start_table_no_zeros)
table_1_no_zeros_hm <- occ_heat_map(table_1_no_zeros)
table_step_2_min_hm <- occ_heat_map(table_step_2_min)
table_step_3_min_hm <- occ_heat_map(table_step_3_min)
table_step_4_min_hm <- occ_heat_map(table_step_4_min)
table_step_5_min_hm <- occ_heat_map(table_step_5_min)

#plot and save
plot_and_save(start_table_no_zeros_hm, width = 15, height = 10)
plot_and_save(table_1_no_zeros_hm, width = 15, height = 10)
plot_and_save(table_step_2_min_hm, width = 15, height = 10)
plot_and_save(table_step_3_min_hm, width = 15, height = 10)
plot_and_save(table_step_4_min_hm, width = 15, height = 10)
plot_and_save(table_step_5_min_hm, width = 15, height = 10)
```

#### Generate tables with probabilities
We apply the bias correction strategies also accounting for the uncertainty they entail.
We let:
* m    be the maximum distance threshold for the language bias TO DO
* l_it be the iterator for the language bias
* s_it be the iterator for the status bias
* g_it be the iterator for the gender bias
* p    be the parameter in the rural population model

```{r bias-corrected df of tables with probabilities}
# set number of iterations
m <- max_distances
l_it_n <- 10
s_it_n <- 5 
g_it_n <- 5  # up to 1000
p <- 0.95     # p<1 

# number of geographical units and occupations
n_gu <- 54 # compute them from province_table_greek_latin_list after the issue with border regions has been corrected
n_occ <- 430

# compute number of rows of the final table
n_rows <- n_gu*n_occ*l_it_n*s_it_n*g_it_n*2

# initialise df occupation tables
bias_corrected_tables_df <- data.frame(
  province = rep("NA",n_rows),
  occupation = NA,
  value = 0,
  minmax = NA)
 
# initialise index
index_start <- 1

for (l_it in 1:l_it_n) {
  # 1. Language
  table_step_1 <- province_table_greek_latin_list[[l_it]]
  # delete the border regions (WILL HAVE TO BE DONE FROM THE START LATER ON)
  table_step_1 <- table_step_1[!rownames(table_step_1)%in%c("Eastern border","North-eastern border"),]

  # 2. status bias
  table_step_2_list <- status_bias_correction_stochastic(table_step_1,3,5,s_it_n)
  for (s_it in 1:s_it_n) {
    table_step_2 <- table_step_2_list[[s_it]]
    
    # 3. gender bias
    for (g_it in 1:g_it_n) {
      gender_ratio <- gender_ratio_v[g_it]
      table_step_3 <- gender_bias(table_step_2,gender_ratio)

      # 4. agricola
      table_step_4_list <- agricola_correction(table_step_3,p)
      table_step_4_min <- table_step_4_list$min
      table_step_4_max <- table_step_4_list$max

      # 5. research bias
      table_step_5_min <- research_correction(table_step_4_min, "min")
      table_step_5_max <- research_correction(table_step_4_max, "max")
      
      # Record information
      step_5_min_df <- as.data.frame(table_step_5_min) %>%
        setNames(c("province","occupation","value"))
      step_5_min_df$minmax <- "min"
      step_5_max_df <- as.data.frame(table_step_5_max) %>%
        setNames(c("province","occupation","value"))
      step_5_max_df$minmax <- "max"
      step_5_minmax_df <- rbind(step_5_min_df,step_5_max_df)
      
      # transform into character
      step_5_minmax_df$province <- as.character(step_5_minmax_df$province)
      step_5_minmax_df$occupation <- as.character(step_5_minmax_df$occupation)
      
      # save in larger df
      print(index_start)
      index_end <- index_start+2*n_gu*n_occ-1
      bias_corrected_tables_df[index_start:index_end,] <-  step_5_minmax_df
      
      # set new index start
      index_start <- index_end +1
    }
  }
  print(l_it)
}

# complete df with info on the iterators
bias_corrected_tables_df$l_it <- rep(1:l_it_n, each = n_gu*n_occ*s_it_n*g_it_n*2)
bias_corrected_tables_df$s_it <- rep(rep(1:s_it_n, each = n_gu*n_occ*g_it_n*2),l_it_n)
bias_corrected_tables_df$g_it <- rep(rep(1:g_it_n, each = n_gu*n_occ*2), s_it_n*l_it_n)

# write csv
write.csv(bias_corrected_tables_df,
  file = "../../results/economic_complexity/bias_corrected_tables.csv")
```

We can extract individual tables as follows
```{r example of bias corrected table}
# extract the first table 
bias_corrected_table_example_df <- bias_corrected_tables_df[1:(n_gu*n_occ),c("province","occupation","value")]

# transform into a matrix
bias_corrected_table_example <- bias_corrected_table_example_df %>%
  pivot_wider(names_from = occupation, values_from = value) %>%
  column_to_rownames("province")
```

# Economic complexity
```{r Translate Michele's code for computing economic complexity}
#Hidalgo-Hausmann classical ECI implementation (based on products and exports)
economic_complexity <- function(mat){
  # Compute product-product and exporter-exporter matrices
  {
    # Transform table into matrix
    M_cp <- mat %>% as.matrix()
    
    # Calculates the revealed comparative advantage (RCA), and binarizes it -- filling True only for the exporters which exported significant quantities of the product (RCA > 1)
    M_cp <- ((t(M_cp) / colSums(M_cp)) %>% t() / (rowSums(M_cp) / sum(M_cp))) > 1
    
    # Sums the number of exporters exporting the product with RCA > 1
    ubiquity <- colSums(M_cp)
    
    # Sums the number of products exported by the exporter with RCA > 1
    diversity <- rowSums(M_cp)
    
    # Column normalized M_cp
    Q <- t(t(M_cp) / ubiquity)
    
    # Row normalized M_cp
    R <- M_cp / diversity
    
    # Square product-product matrix
    Spp <- as.matrix(t(R) %*% Q)
    
    # Square exporter-exporter matrix
    Scc <- as.matrix(Q %*% t(R))
  }
  
  # Calculate the eigenvector of the square product-product matrix and takes the second largest. If it doesn't correlate with ubiquity, multiplies by -1
  {
    # Calculate the eigenvector of the square product-product matrix and take the second largest
    Spp_eigen <- eigen(Spp)
    
    # The eigenvectors are returned in random order, so we need to sort them so that we are sure we're picking the second largest
    id_pci <- order(Spp_eigen$values)
    
    pci <- Re(Spp_eigen$vectors[,id_pci[length(id_pci) - 1]])
    
    # Check correlation and adjust sign if necessary
    if (cor(pci, ubiquity) < 0) {
      pci <- pci * -1
    }
  }
  
  # Calculate the eigenvector of the square exporter-exporter matrix and takes the second largest. If it doesn't correlate with ubiquity, multiplies by -1
  {
    # Calculate the eigenvector of the square product-product matrix and take the second largest
    Scc_eigen <- eigen(Scc)
    
    # The eigenvectors are in random order, so we need to sort them so that we are sure we're picking the second largest
    id_eci <- order(Scc_eigen$values)
    
    eci <- Re(Scc_eigen$vectors[,id_eci[length(id_eci) - 1]])
    
    # Check correlation and adjust sign if necessary
    if (cor(eci, diversity) < 0) {
      eci <- eci * -1
    }
  }
  
  # Compute tables for export
  {
    # identify gu_type
    gu_type <- which_gu(mat)
    
    # Create the pci DataFrame 
    pci_df <- data.frame(colnames(M_cp), pci) %>% setNames(c("occupation","pci"))
    
    # Create the eci DataFrame
    eci_df <- data.frame(rownames(M_cp), eci) %>% setNames(c(gu_type,"eci"))
    
    # pci and eci gets standardized with the same strategy described in the Atlas of Economic Complexity
    pci_df["pci"] = (pci_df["pci"] - mean(pci_df[["pci"]])) / sd(pci_df[["pci"]])
    eci_df["eci"] = (eci_df["eci"] - mean(eci_df[["eci"]])) / sd(eci_df[["eci"]])
    
    # sort the values
    pci_df <- pci_df %>% arrange(pci)
    eci_df <- eci_df %>% arrange(eci)
    
    result <- list(
      pci = pci_df,
      eci = eci_df
    )
    
    return(result)
  }
}
```

```{r compute and save the values from the tables}
for (i in 1:500) {
  # start index
  start_index <- (i-1)*(n_gu*n_occ)+1
  end_index <- i*(n_gu*n_occ)

  # extract the ith table 
  table_i_df <-   bias_corrected_tables_df[
    start_index:end_index,
    c("province","occupation","value")
    ]

  # transform into a matrix
  table_i <- table_i_df %>%
    pivot_wider(names_from = occupation, values_from = value) %>%
    column_to_rownames("province")
  
  # compute economic complexity
  result_i <- economic_complexity(table_i)
  
  # extract and save results
  pci_i <- result_i$pci
  eci_i <- result_i$eci
  
  # Write tables to file
  write.csv(pci_i, str_c("../../results/economic_complexity/pci_eci/pci_",i,".csv"))
  write.csv(eci_i, str_c("../../results/economic_complexity/pci_eci/eci_",i,".csv"))
    
  # print progress 
  print(i)
}
```

```{r compute and save the values from the tables as a single dataframe}
# initialise dataframes (RK: pci and eci will no longer be ordered)
pci_combined_df <- data.frame(occupation = colnames(bias_corrected_table_example))
eci_combined_df <- data.frame(province = rownames(bias_corrected_table_example))

for (i in 1:500) {
  # start index
  start_index <- (i-1)*(n_gu*n_occ)+1
  end_index <- i*(n_gu*n_occ)

  # extract the ith table 
  table_i_df <-   bias_corrected_tables_df[
    start_index:end_index,
    c("province","occupation","value")
    ]

  # transform into a matrix
  table_i <- table_i_df %>%
    pivot_wider(names_from = occupation, values_from = value) %>%
    column_to_rownames("province")
  
  # compute economic complexity
  result_i <- economic_complexity(table_i)
  
  # extract and save results
  pci_i <- result_i$pci %>% setNames(c("occupation",str_c("pci_",i)))
  eci_i <- result_i$eci %>% setNames(c("province",str_c("eci_",i)))
  
  # combine with previous table
  pci_combined_df <- left_join(pci_combined_df,pci_i,by=join_by("occupation"))
  eci_combined_df <- left_join(eci_combined_df,eci_i,by=join_by("province"))

  # print progress
  if (i%%50==0){
    print(i)
  }
}

# Write tables to file
write.csv(pci_combined_df, "../../results/economic_complexity/pci_eci/pci_combined.csv")
write.csv(eci_combined_df, "../../results/economic_complexity/pci_eci/eci_combined.csv")
```

```{r heat map of ranks}
heatmap(eci_combined_df[1:54,2:501] %>% as.matrix() %>% apply(2,rank))
```

# Assessment
Here we study on effects of bias correction on occupation tables and economic complexity

First, define a set of auxiliary functions that transform a list of tables into an array (for storage) and compute basic stats.
```{r functions for summary statistics}
# function to transform a list of tables into an array
tbl_list_to_array <- function(tbl_list){
  # check that the input format is valid
  if (!("list" %in% class(tbl_list) & "table" %in% class(tbl_list[[1]]))){
    stop("Invalid input. The input must be a list of tables")
  }
  
  # define a consistent row and column order
  row_order <- rownames(tbl_list[[1]])
  col_order <- colnames(tbl_list[[1]])
  
  # Reorder rows and columns for each table in the list
  aligned_tables <- lapply(tbl_list, 
                           function(tbl) {
                             tbl[row_order, col_order]})
  
  # store as array
  tbl_array <- simplify2array(aligned_tables)

  # return tables as array
  return(tbl_array)
}

# the following function computes basic table stats
array_stats <- function(arr){
  
  # initialise stat list
  stats <- list()
  
  # Compute statistics as before
  stats$mean <- apply(arr, c(1, 2), mean)
  stats$median <- apply(arr, c(1, 2), median)
  stats$sd <- apply(arr, c(1, 2), sd)
  stats$quartiles <- apply(arr, c(1, 2), quantile, probs = c(0,0.25, 0.5, 0.75,1))

  # return list of stats
  return(stats)
}

# the following function combined the above
list_stats <- function(tbl_list){
  stats <- tbl_list %>% tbl_list_to_array() %>% array_stats
  return(stats)
}
```

The following function computes the basic stats after each correction process individually to the original occupation tables
```{r effects of individual bias corrections on tables}
individual_bias_stats <- function(original_tb){
  # identify gu type
  gu_type <- which_gu(original_tb)
  
  # initialise stats
  gu_stats <- list()
  
  # add start table
  gu_stats$original <- original_tb
  
  # language bias
  language_stats <- list()
  for (m in paste0("r_",max_distances)){
    language_stats[[m]] <- get(paste0(gu_type,"_table_greek_latin_lists"))[[m]] %>% 
      list_stats()
  }
  gu_stats[["language"]] <- language_stats
  print("language done")
  
  # status bias
  status_tbs <- status_bias_correction_stochastic(original_tb,15,30,n_samples=1000)
  gu_stats[["status"]] <- list_stats(status_tbs)
  print("status done")

  # gender bias
  gender_tbs <- lapply(gender_ratio_v, 
                       function(gender_ratio) gender_bias(original_tb, gender_ratio))
  gu_stats[["gender"]] <- list_stats(gender_tbs)
  print("gender done")

  # agricola
  agricola_tbs <- lapply(seq(0,1,by=0.001),
                         function(s) agricola_correction(original_tb,p=0.95,s))
  gu_stats[["agricola"]] <- list_stats(agricola_tbs)
  print("agricola done")

  # research
  research_tbs <- lapply(seq(0,1,by=0.001),
                         function(p) research_correction(original_tb,p))
  gu_stats[["research"]] <- list_stats(research_tbs)
  print("research done")

  return(gu_stats)
}
```

We apply the function to compute basic stats for provinces and countries
```{r compute the stats for provinces and countries}
province_individual_bias_stats <- individual_bias_stats(province_table)
# save the results
saveRDS(province_individual_bias_stats, file = "../../results/economic_complexity/province_individual_bias_stats.rds")

country_individual_bias_stats <- individual_bias_stats(country_table)
# save the results
saveRDS(country_individual_bias_stats, file = "../../results/economic_complexity/country_individual_bias_stats.rds")
```

We plot basic stats for each correction process as a heatmap
```{r heatmaps of mean values}
for (gu in c("province","country")) {
  bias_tb <- get(paste0(gu,"_individual_bias_stats"))
  
  for (bias in c("language","status","gender","agricola","research")) {
    tb <- bias_tb[[bias]]
    if (bias == "language"){
      for (m in paste0("r_",max_distances)) {
        # extract table
        tb_m <- tb[[m]]
        # extract stats
        tb_m_min <- tb_m$quartile[1,,] %>% as.table()
        tb_m_median <- tb_m$median %>% as.table()
        tb_m_max <- tb_m$quartile[5,,] %>% as.table()
        tb_m_mean <- tb_m$mean %>% as.table()
        tb_m_sd <- tb_m$sd %>% as.table()
        
        # plot
        for (stat in c("min","median","max","mean","sd")) {
          tb_m_stat <- get(paste0("tb_m_",stat))
          plot_and_save(occ_heat_map(tb_m_stat),
                        title = 
                        paste0(gu,"_",bias,"_bias_correction_",m,"_", stat,"_heatmap"),
                        width=15, height=10)
        }
      } 
    } else {
      # extract stats
      tb_min <- tb$quartile[1,,] %>% as.table()
      tb_median <- tb$median %>% as.table()
      tb_max <- tb$quartile[5,,] %>% as.table()
      tb_mean <- tb$mean %>% as.table()
      tb_sd <- tb$sd %>% as.table()
      
      # plot
      for (stat in c("min","median","max","mean","sd")) {
        tb_stat <- get(paste0("tb_",stat))
        plot_and_save(occ_heat_map(tb_stat),
                      title = 
                        paste0(gu,"_",bias,"_bias_correction_",stat,"_heatmap"),
                      width=15, height=10)}
    }
    print(bias)
  }
  print(gu)
}
```

We compute the impact of individual correction biases on the economic complexity
```{r effects of individual bias corrections on eci}
# this function computes the eci for a list of occupation tables
eci_list <- function(occ_tb_list){
  list_length <- length(occ_tb_list)
  ecis <- lapply(
      seq_len(list_length),
      function(i){
        occ_tb_i <- occ_tb_list[[i]]
        # ensure that the occupation table has no zero rows
        occ_tb_i <- occ_tb_i[rowSums(occ_tb_i)>0,]
        economic_complexity(occ_tb_i)$eci}
      )
  return(ecis)
}

# this function computes stats from a list of eci
eci_stats <- function(eci_list, gu_type){
  eci_list_l <- length(eci_list)
  # trasform list into df
  ecis_df <- reduce(eci_list, full_join, by = gu_type) %>%
    setNames(c(gu_type,paste0("eci_",seq(eci_list_l))))
  # add stats
  ecis_df <- ecis_df %>%
      rowwise() %>%
      mutate(
        mean = mean(c_across(starts_with("eci")), na.rm = TRUE),
        sd = sd(c_across(starts_with("eci")), na.rm = TRUE),
        min = quantile(c_across(starts_with("eci")), 0, na.rm = TRUE),
        Q1 = quantile(c_across(starts_with("eci")), 0.25, na.rm = TRUE),
        median = quantile(c_across(starts_with("eci")), 0.5, na.rm = TRUE),
        Q3 = quantile(c_across(starts_with("eci")), 0.75, na.rm = TRUE),
        max = quantile(c_across(starts_with("eci")), 1, na.rm = TRUE)) %>%
      ungroup()
  
  return(ecis_df)
}

# this function computes eci stats after each bias correction procedure
individual_bias_eci_stats <- function(original_tb){
  # identify gu type
  gu_type <- which_gu(original_tb)
  
  # initialise stats
  gu_stats <- list()
  
  # ensure that the occupation table has no zero rows
  original_tb <- original_tb[rowSums(original_tb)>0,]
  
  # compute values for the original table
  gu_stats$original <- economic_complexity(original_tb)
  
  # language bias
  language_stats <- list()
  for (m in paste0("r_",max_distances[1:2])){
    gu_table_greek_latin_lists <- get(paste0(gu_type,"_table_greek_latin_lists"))[[m]][1:100]
    # compute eci for this list
    ecis_m <- eci_list(gu_table_greek_latin_lists)
    language_stats[[m]] <- eci_stats(ecis_m,gu_type)
    }
  gu_stats[["language"]] <- language_stats
  print("language done")
  
  # status bias
  status_tbs <- status_bias_correction_stochastic(original_tb,15,30,n_samples=100)
  status_ecis <- eci_list(status_tbs)
  gu_stats[["status"]] <- eci_stats(status_ecis,gu_type)
  print("status done")

  # gender bias
  gender_tbs <- lapply(gender_ratio_v[1:100], 
                       function(gender_ratio) gender_bias(original_tb, gender_ratio))
  gender_ecis <- eci_list(gender_tbs)
  gu_stats[["gender"]] <- eci_stats(gender_ecis,gu_type)
  print("gender done")

  # agricola
  agricola_tbs <- lapply(seq(0,1,by=0.01),
                         function(s) agricola_correction(original_tb,p=0.95,s))
  agricola_ecis <- eci_list(agricola_tbs)
  gu_stats[["agricola"]] <- eci_stats(agricola_ecis,gu_type)
  print("agricola done")

  # research
  research_tbs <- lapply(seq(0,1,by=0.01),
                         function(p) research_correction(original_tb,p))
  research_ecis <- eci_list(research_tbs)
  gu_stats[["research"]] <- eci_stats(research_ecis,gu_type)
  print("research done")

  return(gu_stats)
}
```

We apply the function to compute basic stats for the eci of provinces and countries
```{r compute the stats for provinces and countries eci}
province_individual_bias_eci_stats <- individual_bias_eci_stats(province_table)
# save the results
saveRDS(province_individual_bias_eci_stats, file = "../../results/economic_complexity/province_individual_bias_eci_stats.rds")

country_individual_bias_eci_stats <- individual_bias_eci_stats(country_table)
# save the results
saveRDS(country_individual_bias_eci_stats, file = "../../results/economic_complexity/country_individual_bias_eci_stats.rds")
```

Counts
```{r dfs of counts of inscriptions and occupations}
# number of occupations in each province
n_occupations_in_provinces <- data.frame(
  province = rownames(province_table), 
  n_occupations = rowSums(province_table)
  ) 
# number of occupations in each country
n_occupations_in_countries <- data.frame(
  country = rownames(country_table), 
  n_occupations = rowSums(country_table)
  ) 

# create a table for export
n_inscriptions_and_occupations_in_provinces <- n_latin_inscriptions_in_provinces %>% 
  left_join(n_inscriptions_w_occupations_in_provinces) %>% 
  left_join(n_occupations_in_provinces) %>% 
  left_join(n_greek_inscriptions_in_provinces)
n_inscriptions_and_occupations_in_countries <- n_latin_inscriptions_in_countries %>% 
  left_join(n_inscriptions_w_occupations_in_countries) %>% 
  left_join(n_occupations_in_countries) %>% 
  left_join(n_greek_inscriptions_in_countries)

# set NAs to 0
n_inscriptions_and_occupations_in_provinces[is.na(n_inscriptions_and_occupations_in_provinces)] <- 0
n_inscriptions_and_occupations_in_countries[is.na(n_inscriptions_and_occupations_in_countries)] <- 0

# export
#write.csv(n_inscriptions_and_occupations_in_provinces,
#          "../../results/economic_complexity/n_inscriptions_and_occupations_in_provinces.csv")
#write.csv(n_inscriptions_and_occupations_in_countries,
#          "../../results/economic_complexity/n_inscriptions_and_occupations_in_countries.csv")

# map counts by province
provinces_with_counts <- left_join(provinces,
                                   n_inscriptions_and_occupations_in_provinces,
                                   join_by(province))

# to filter out excessively small values
#provinces_with_occ_counts$counts[provinces_with_occ_counts$counts<1]<-0

# define map
map_plot <- ggplot(data = provinces_with_counts) +
            geom_sf(aes(fill = log(n_occupations))) +
             coord_sf(
                xlim = c(-15,40), 
                ylim = c(20,60)) +
            scale_fill_gradient(low = "white", high = "red", name = "Value") +
            theme_minimal() +
            labs(title = "Log of number of occupations per province")
plot_and_save(map_plot)

```

On the basis of the above counts, we establish a filter
```{r filter regions with more than 1000 Latin inscriptions}
# provinces
filtered_provinces <- n_inscriptions_and_occupations_in_provinces$province[n_inscriptions_and_occupations_in_provinces$n_latin_inscriptions>1000]

# countries
filtered_countries <- n_inscriptions_and_occupations_in_countries$country[n_inscriptions_and_occupations_in_countries$n_latin_inscriptions>1000]
```

We then compute the stats for filtered provinces and countries
```{r compute the stats for filtered provinces and countries eci}
# provinces
filtered_province_table <- province_table[filtered_provinces,]
filtered_province_individual_bias_eci_stats <- individual_bias_eci_stats(filtered_province_table)
# save the results
saveRDS(filtered_province_individual_bias_eci_stats, file = "../../results/economic_complexity/filtered_province_individual_bias_eci_stats.rds")

# countries
filtered_country_table <- country_table[filtered_countries,]
filtered_country_individual_bias_eci_stats <- individual_bias_eci_stats(filtered_country_table)
# save the results
saveRDS(filtered_country_individual_bias_eci_stats, file = "../../results/economic_complexity/filtered_country_individual_bias_eci_stats.rds")
```

We plot these as boxplots 
```{r boxplot of effects of individual biases on eci}
for (exclude_Rome_gu in c(TRUE,FALSE)){
  for (gu in c("province","country")) {
  bias_tb <- get(paste0(gu,"_individual_bias_eci_stats"))
  #bias_tb <- get(paste0("filtered_",gu,"_individual_bias_eci_stats"))

  for (bias in c("language","status","gender","agricola","research")) {
    tb <- bias_tb[[bias]]
    if (bias == "language"){
      for (m in paste0("r_",max_distances[1:2])) {
        # extract eci
        eci_m <- tb[[m]]
        original_eci <- bias_tb$original$eci
        
        # select eci values 
        eci_values <- eci_m[,c(gu,paste0("eci_",1:100))]
        if (exclude_Rome_gu){
          eci_values <- eci_values[!eci_values[[gu]] %in% 
                                     c("ITA","Latium et Campania (Regio I)"),]
          original_eci <- original_eci[!original_eci[[gu]] %in% 
                                         c("ITA","Latium et Campania (Regio I)"),]
          }
        
        # tranform into a long df for ggplot boxplot
        eci_long <- eci_values %>%
          pivot_longer(cols = -gu, names_to = "sample", values_to = "eci")
        
        # sort the gu names in ascending order of median eci  
        gu_order <- eci_m[[gu]][order(eci_m$median)]
        eci_long[[gu]] <- factor(eci_long[[gu]], levels = gu_order)
        
        # define title
        boxplot_title = paste0(
          paste0("Eci by ", gu," based on ", bias," bias correction ",m),
          ifelse(exclude_Rome_gu,", excluding Rome's region","")) 
        
        # define boxplot
        eci_boxplot <- ggplot(eci_long, aes(x = !!sym(gu), y = eci)) +
          geom_boxplot(fill= "black", alpha=0.2) +
          geom_point(data = original_eci, 
                     aes(x = !!sym(gu), y = eci, color = "original eci"), size = 2) +
          scale_color_manual(values = "red", 
                     name = "Legend", 
                     labels = "original eci") +
          theme_minimal() +
          labs(title = boxplot_title, x = sym(gu), y = "eci") +
          theme(axis.text.x = element_text(angle = 45, hjust = 1))
        
        # save
        boxplot_name <- paste0(
          paste0(gu,"_",bias,"_bias_correction_eci_boxplot_",m),
          ifelse(exclude_Rome_gu,"_excluding_Rome_s_region","")) 
        # boxplot_name <- paste0(
        #  paste0("filtered_",gu,"_",bias,"_bias_correction_eci_boxplot_",m),
        #  ifelse(exclude_Rome_gu,"_excluding_Rome_s_region","")) 
        plot_and_save(eci_boxplot,
                      title = boxplot_name,
                      height = 5,
                      width = 10)
      } 
    } else {
        # extract eci values 
        eci_values <- tb[,c(gu,paste0("eci_",1:100))]
        original_eci <- bias_tb$original$eci
        
        #select eci values
         if (exclude_Rome_gu){
           eci_values <- eci_values[!eci_values[[gu]] %in% 
                                      c("ITA","Latium et Campania (Regio I)"),]
           original_eci <- original_eci[!original_eci[[gu]] %in% 
                                          c("ITA","Latium et Campania (Regio I)"),]
           }
      
        # tranform into a long df for ggplot boxplot
        eci_long <- eci_values %>%
          pivot_longer(cols = -gu, names_to = "sample", values_to = "eci")
        
        # sort the gu names in ascending order of median eci  
        gu_order <- tb[[gu]][order(tb$median)]
        eci_long[[gu]] <- factor(eci_long[[gu]], levels = gu_order)
        
        # define title
         boxplot_title = paste0(
          paste0("Eci by ", gu," based on ", bias," bias correction"),
          ifelse(exclude_Rome_gu,", excluding Rome's region","")) 
        
        # define boxplot
        eci_boxplot <- ggplot(eci_long, aes(x = !!sym(gu), y = eci)) +
          geom_boxplot(fill= "black", alpha=0.2) +
          geom_point(data = original_eci, 
                     aes(x = !!sym(gu), y = eci, color = "original eci"), size = 2) +
          scale_color_manual(values = "red", 
                     name = "Legend", 
                     labels = "original eci") +
          theme_minimal() +
          labs(title = boxplot_title, x = sym(gu), y = "eci") +
          theme(axis.text.x = element_text(angle = 45, hjust = 1))
        
        # save
        boxplot_name <- paste0(
          paste0(gu,"_",bias,"_bias_correction_eci_boxplot"),
          ifelse(exclude_Rome_gu,"_excluding_Rome_s_region","")) 
        #boxplot_name <- paste0(
        # paste0("filtered_",gu,"_",bias,"_bias_correction_eci_boxplot"),
        #  ifelse(exclude_Rome_gu,"_excluding_Rome_s_region","")) 
        plot_and_save(eci_boxplot,
                      title = boxplot_name,
                      height = 5,
                      width = 10)
    }
    print(bias)
  }
  print(gu)
}
}
```

We now write function combining all the required correction strategies
```{r eci stats after all bias corrections}
# this function computes eci stats after all bias correction procedures have been applied
all_bias_eci_stats <- function(original_tb,
                               language_it = 10, # cases considered
                               status_it = 2, 
                               gender_it = 2,
                               agricola_it = 5,
                               research_it = 5){
  # identify gu type
  gu_type <- which_gu(original_tb)
  
  # initialise stats
  gu_stats <- list()
  
  # ensure that the occupation table has no zero rows
  original_tb <- original_tb[rowSums(original_tb)>0,]
  
  # compute values for the original table
  gu_stats$original <- economic_complexity(original_tb)
  
  # loop and add
  for (m in paste0("r_",max_distances[c(1,2)])){
    languages_tbl_m <- get(paste0(gu_type,"_table_greek_latin_lists"))[[m]][seq_len(language_it)]
    bias_type_m <- paste0("language_r_",m)
    
    for (l_it in seq_len(language_it)) {
      tb_l_it <- languages_tbl_m[[l_it]]
      status_tbs <- status_bias_correction_stochastic(tb_l_it,15,30,n_samples=status_it)
      bias_type_l <- paste0(bias_type_m,"_l_it_",l_it,"_status")
      
      for (s_it in seq_len(status_it)) {
        tb_s_it <- status_tbs[[s_it]]
        gender_tbs <- lapply(gender_ratio_v, 
                       function(gender_ratio) gender_bias(tb_s_it, gender_ratio))
        bias_type_s <- paste0(bias_type_l,"_s_it_",s_it,"_gender")

        for (g_it in seq_len(gender_it)) {
          tb_g_it <- gender_tbs[[g_it]]
          agricola_tbs <- lapply(seq(0,1,length.out=agricola_it),
                         function(s) agricola_correction(tb_g_it,p=0.95,s))
          bias_type_g <- paste0(bias_type_s,"_g_it_",g_it,"_agricola")

          for (a_it in seq_len(agricola_it)) {
            tb_a_it <- agricola_tbs[[a_it]]
            research_tbs <- lapply(seq(0,1,length.out=research_it),
                                   function(p) research_correction(tb_a_it,p))
            bias_type_a <- paste0(bias_type_g,"_a_it_",a_it,"_research")
            research_ecis <- eci_list(research_tbs)
            gu_stats[[bias_type_a]] <- eci_stats(research_ecis,gu_type)
            print(bias_type_a)
          }
        } 
      }
    }
  }
  return(gu_stats)
}

# this function does the same, without language bias
all_but_language_bias_eci_stats <- function(original_tb,
                                            status_it = 2, # cases considered
                                            gender_it = 2,
                                            agricola_it = 5,
                                            research_it = 5){
  # identify gu type
  gu_type <- which_gu(original_tb)
  
  # initialise stats
  gu_stats <- list()
  
  # ensure that the occupation table has no zero rows
  original_tb <- original_tb[rowSums(original_tb)>0,]
  
  # compute values for the original table
  gu_stats$original <- economic_complexity(original_tb)
  
  # apply status correction
  status_tbs <- status_bias_correction_stochastic(original_tb,15,30,n_samples=status_it)

  # loop and add
  for (s_it in seq_len(status_it)) {
    tb_s_it <- status_tbs[[s_it]]
    gender_tbs <- lapply(gender_ratio_v, 
                   function(gender_ratio) gender_bias(tb_s_it, gender_ratio))
    bias_type_s <- paste0("status_s_it_",s_it,"_gender")

    for (g_it in seq_len(gender_it)) {
      tb_g_it <- gender_tbs[[g_it]]
      agricola_tbs <- lapply(seq(0,1,length.out=agricola_it),
                     function(s) agricola_correction(tb_g_it,p=0.95,s))
      bias_type_g <- paste0(bias_type_s,"_g_it_",g_it,"_agricola")

      for (a_it in seq_len(agricola_it)) {
        tb_a_it <- agricola_tbs[[a_it]]
        research_tbs <- lapply(seq(0,1,length.out=research_it),
                               function(p) research_correction(tb_a_it,p))
        bias_type_a <- paste0(bias_type_g,"_a_it_",a_it,"_research")
        research_ecis <- eci_list(research_tbs)
        gu_stats[[bias_type_a]] <- eci_stats(research_ecis,gu_type)
        print(bias_type_a)
      }
    } 
  }
  return(gu_stats)
}
```

Apply all corrections to the original tables
```{r compute the stats for provinces and countries eci after all bias corrections}
# provinces
province_all_bias_eci_stats <- all_bias_eci_stats(province_table)
# save the results
saveRDS(province_all_bias_eci_stats, file = "../../results/economic_complexity/province_all_bias_eci_stats.rds")

# countries
country_all_bias_eci_stats <- all_bias_eci_stats(country_table)
# save the results
saveRDS(country_all_bias_eci_stats, file = "../../results/economic_complexity/country_all_bias_eci_stats.rds")
```

Apply all corrections but language to the filtered tables
```{r compute the stats for filtered provinces and countries eci after all but language bias corrections}
# provinces
filtered_province_table <- province_table[filtered_provinces,]
filtered_province_all_but_language_bias_eci_stats <- all_but_language_bias_eci_stats(filtered_province_table)
# save the results
saveRDS(filtered_province_all_but_language_bias_eci_stats, file = "../../results/economic_complexity/filtered_province_all_but_language_bias_eci_stats.rds")

# countries
filtered_country_table <- country_table[filtered_countries,]
filtered_country_all_but_language_bias_eci_stats <- all_but_language_bias_eci_stats(filtered_country_table)
# save the results
saveRDS(filtered_country_all_but_language_bias_eci_stats, file = "../../results/economic_complexity/filtered_country_all_but_language_bias_eci_stats.rds")
```

Alternative corrective functions
```{r eci stats after all bias corrections}
# this function computes eci stats after all bias correction procedures have been applied
all_bias_eci_df <- function(original_tb,
                            language_it = 10, # cases considered
                            status_it = 2, 
                            gender_it = 2,
                            agricola_it = 5,
                            research_it = 5){
  # identify gu type
  gu_type <- which_gu(original_tb)
  
  # ensure that the occupation table has no zero rows
  original_tb <- original_tb[rowSums(original_tb)>0,]
  
  #retrieve language tables
  languages_tbls <- get(paste0(gu_type,"_table_greek_latin_lists"))
  
  # initialise bias dataframe
  case_r_0 <- languages_tbls$r_0$it_1
  case_r_0.001 <- languages_tbls$r_0.001$it_1
  nrows_case_r_0 <- nrow(case_r_0[rowSums(case_r_0)>0,])
  nrows_case_r_0.001 <- nrow(case_r_0.001[rowSums(case_r_0.001)>0,])

  nrows <- language_it*status_it*gender_it*agricola_it*research_it*(nrows_case_r_0+nrows_case_r_0.001)
  bias_df <- data.frame(matrix(NA, ncol = 8, nrow = nrows)) %>% 
    setNames(c(gu_type,"m","l_it","s_it","g_it","a_it","r_it","eci"))
  
  # define indices for filling the df
  index_start <- 1
    
  # loop and add
  for (m in paste0("r_",max_distances[c(1,2)])){
    languages_tbl_m <- languages_tbls[[m]][seq_len(language_it)]
    nrows_case <- get(paste0("nrows_case_",m))
    
    for (l_it in seq_len(language_it)) {
      tb_l_it <- languages_tbl_m[[l_it]]
      status_tbs <- status_bias_correction_stochastic(tb_l_it,15,30,n_samples=status_it)

      for (s_it in seq_len(status_it)) {
        tb_s_it <- status_tbs[[s_it]]
        gender_tbs <- lapply(gender_ratio_v, 
                       function(gender_ratio) gender_bias(tb_s_it, gender_ratio))

        for (g_it in seq_len(gender_it)) {
          tb_g_it <- gender_tbs[[g_it]]
          agricola_tbs <- lapply(seq(0,1,length.out=agricola_it),
                         function(s) agricola_correction(tb_g_it,p=0.95,s))

          for (a_it in seq_len(agricola_it)) {
            tb_a_it <- agricola_tbs[[a_it]]
            research_tbs <- lapply(seq(0,1,length.out=research_it),
                                   function(p) research_correction(tb_a_it,p))
            research_ecis <- eci_list(research_tbs)
            
            for (r_it in seq(research_it)){
              eci_r_it <- research_ecis[[r_it]]
              
              # define end index
              index_end <- index_start+nrows_case-1
              
              # update dataframes 
              bias_df[index_start:index_end,c(gu_type,"eci")] <- eci_r_it
              bias_df[index_start:index_end,c("m","l_it","s_it","g_it","a_it","r_it")]<-
                rep(c(m,l_it,s_it,g_it,a_it,r_it), each = nrows_case)
              
              # update start index
              # find indices
              index_start <- index_end +1
              
              # check progress
              # print(paste0(index_end," out of ",nrows))
            }
          }
        } 
      }
    }
  }
  return(bias_df)
}

# this function does the same, without language bias 
all_but_language_bias_eci_df <- function(original_tb,
                                            status_it = 2, # cases considered
                                            gender_it = 2,
                                            agricola_it = 5,
                                            research_it = 5){
  # identify gu type
  gu_type <- which_gu(original_tb)
  
  # ensure that the occupation table has no zero rows
  original_tb <- original_tb[rowSums(original_tb)>0,]
  
  # n_rows
  nrow_case <- nrow(original_tb)
  nrows <- status_it*gender_it*agricola_it*research_it*nrow_case
  
  # initialise result df
  bias_df <- data.frame(matrix(NA, ncol = 6, nrow = nrows)) %>% 
    setNames(c(gu_type,"s_it","g_it","a_it","r_it","eci"))
  
  # define indices for filling the df
  index_start <- 1
  
  # apply status correction
  status_tbs <- status_bias_correction_stochastic(original_tb,15,30,n_samples=status_it)

  # loop and add
  for (s_it in seq_len(status_it)) {
    tb_s_it <- status_tbs[[s_it]]
    gender_tbs <- lapply(gender_ratio_v, 
                   function(gender_ratio) gender_bias(tb_s_it, gender_ratio))

    for (g_it in seq_len(gender_it)) {
      tb_g_it <- gender_tbs[[g_it]]
      agricola_tbs <- lapply(seq(0,1,length.out=agricola_it),
                     function(s) agricola_correction(tb_g_it,p=0.95,s))

      for (a_it in seq_len(agricola_it)) {
        tb_a_it <- agricola_tbs[[a_it]]
        research_tbs <- lapply(seq(0,1,length.out=research_it),
                               function(p) research_correction(tb_a_it,p))
         research_ecis <- eci_list(research_tbs)
         
         for (r_it in seq(research_it)){
              eci_r_it <- research_ecis[[r_it]]
              
              # define end index
              index_end <- index_start+nrow_case-1
              
              # update dataframes 
              bias_df[index_start:index_end,c(gu_type,"eci")] <- eci_r_it
              bias_df[index_start:index_end,c("s_it","g_it","a_it","r_it")]<-
                rep(c(s_it,g_it,a_it,r_it), each = nrow_case)
              
              # update start index
              # find indices
              index_start <- index_end +1
              
              print(paste0(index_end," out of ",nrows))
            }
        
      }
    } 
  }
  return(bias_df)
}
```

Apply alternative corrections to the original tables
```{r compute the eci for provinces and countries after all bias corrections}
# provinces
province_all_bias_eci_df <- all_bias_eci_df(province_table)
# save the results
write.csv(province_all_bias_eci_df, file = "../../results/economic_complexity/province_all_bias_eci_df.csv")

# countries
country_all_bias_eci_df <- all_bias_eci_df(country_table)
# save the results
write.csv(country_all_bias_eci_df, file = "../../results/economic_complexity/country_all_bias_eci_df.csv")
```

Apply alternative corrections but language to the filtered tables
```{r compute the eci for filtered provinces and countries after all but language bias corrections}
# provinces
filtered_province_table <- province_table[filtered_provinces,]
filtered_province_all_but_language_bias_eci_df <- all_but_language_bias_eci_df(filtered_province_table)
# save the results
write.csv(filtered_province_all_but_language_bias_eci_df, file = "../../results/economic_complexity/filtered_province_all_but_language_bias_eci_df.csv")

# countries
filtered_country_table <- country_table[filtered_countries,]
filtered_country_all_but_language_bias_eci_df <- all_but_language_bias_eci_df(filtered_country_table)
# save the results
write.csv(filtered_country_all_but_language_bias_eci_df, file = "../../results/economic_complexity/filtered_country_all_but_language_bias_eci_df.csv")
```

Use the tables to create boxplots
```{r ecis boxplots}
# with all gu, and language bias (speculative)
for (exclude_Rome_gu in c(TRUE,FALSE)) {
  for (gu in c("province","country")) {
     for (m in paste0("r_",max_distances[1:2])) {
       # extract eci
       eci_m <- get(paste0(gu,"_all_bias_eci_df"))
       eci_values <- eci_m[eci_m$m==m,c(gu,"eci")]
       original_eci <- get(paste0(gu,"_all_bias_eci_stats"))$original$eci
        
       # specify plot
       boxplot_title <- paste0("Bias-corrected eci by ",gu)
       boxplot_title <- ifelse(m=="r_0",
                               paste0(boxplot_title,", language correction without neighbours"),
                               paste0(boxplot_title,", language correction with neighbours"))

       # select eci values 
       if (exclude_Rome_gu){
          eci_values <- eci_values[!eci_values[[gu]] %in% 
                                     c("ITA","Latium et Campania (Regio I)"),]
          original_eci <- original_eci[!original_eci[[gu]] %in% 
                                         c("ITA","Latium et Campania (Regio I)"),]
          boxplot_title <- paste0(boxplot_title, ", excluding Rome's region")
       }
       
       
      # define boxplot
      eci_boxplot <- ggplot(eci_values, 
                            aes(x = reorder(!!sym(gu), eci, FUN = median), y = eci)) +
        geom_boxplot(fill= "black", alpha=0.2) +
        geom_point(data = original_eci, 
                   aes(x = !!sym(gu), y = eci, color = "original eci"), size = 2) +
        scale_color_manual(values = "red", 
                   name = "Legend", 
                   labels = "original eci") +
        theme_minimal() +
        labs(title = boxplot_title, x = sym(gu), y = "eci") +
        theme(axis.text.x = element_text(angle = 45, hjust = 1))

          
      # save
      boxplot_name <- str_replace_all(boxplot_title," ","_")
      plot_and_save(eci_boxplot,
                    title = boxplot_name,
                    height = 5,
                    width = 10)
      
      # check progress
      print(boxplot_name)
    }
  }
}

# with filtered gu, and without language (conservative)
for (exclude_Rome_gu in c(TRUE,FALSE)) {
  for (gu in c("province","country")) {
    # extract eci
     eci_values <- get(paste0("filtered_",gu,"_all_but_language_bias_eci_df"))
     eci_values <- eci_values[,c(gu,"eci")]
     original_eci <- get(paste0("filtered_",gu,"_all_but_language_bias_eci_stats"))$original$eci
      
     # specify plot
     boxplot_title <- paste0("Bias-corrected eci by selected ",gu)

     # select eci values 
     if (exclude_Rome_gu){
        eci_values <- eci_values[!eci_values[[gu]] %in% 
                                   c("ITA","Latium et Campania (Regio I)"),]
        original_eci <- original_eci[!original_eci[[gu]] %in% 
                                       c("ITA","Latium et Campania (Regio I)"),]
        boxplot_title <- paste0(boxplot_title, ", excluding Rome's region")
     }
     
     
    # define boxplot
    eci_boxplot <- ggplot(eci_values, 
                          aes(x = reorder(!!sym(gu), eci, FUN = median), y = eci)) +
      geom_boxplot(fill= "black", alpha=0.2) +
      geom_point(data = original_eci, 
                 aes(x = !!sym(gu), y = eci, color = "original eci"), size = 2) +
      scale_color_manual(values = "red", 
                 name = "Legend", 
                 labels = "original eci") +
      theme_minimal() +
      labs(title = boxplot_title, x = sym(gu), y = "eci") +
      theme(axis.text.x = element_text(angle = 45, hjust = 1))

        
    # save
    boxplot_name <- str_replace_all(boxplot_title," ","_")
    plot_and_save(eci_boxplot,
                  title = boxplot_name,
                  height = 5,
                  width = 10)
    
    # check progress
    print(boxplot_name)
  }
}
```

Produce occupation tables instead
```{r functions for occupation tables all biases}
# this function computes tbls after all bias correction procedures have been applied
all_bias_tbs <- function(original_tb,
                           language_it = 10, # cases considered
                            status_it = 2, 
                            gender_it = 2,
                            agricola_it = 5,
                            research_it = 5){
  # identify gu type
  gu_type <- which_gu(original_tb)
  
  # ensure that the occupation table has no zero rows
  original_tb <- original_tb[rowSums(original_tb)>0,]
  
  #retrieve language tables
  languages_tbls <- get(paste0(gu_type,"_table_greek_latin_lists"))
  
  # initialise tb list
  tbs_list <- list()
    
  # loop and add
  for (m in paste0("r_",max_distances[c(1,2)])){
    languages_tbl_m <- languages_tbls[[m]][seq_len(language_it)]

    for (l_it in seq_len(language_it)) {
      tb_l_it <- languages_tbl_m[[l_it]]
      status_tbs <- status_bias_correction_stochastic(tb_l_it,15,30,n_samples=status_it)

      for (s_it in seq_len(status_it)) {
        tb_s_it <- status_tbs[[s_it]]
        gender_tbs <- lapply(gender_ratio_v, 
                       function(gender_ratio) gender_bias(tb_s_it, gender_ratio))

        for (g_it in seq_len(gender_it)) {
          tb_g_it <- gender_tbs[[g_it]]
          agricola_tbs <- lapply(seq(0,1,length.out=agricola_it),
                         function(s) agricola_correction(tb_g_it,p=0.95,s))

          for (a_it in seq_len(agricola_it)) {
            tb_a_it <- agricola_tbs[[a_it]]
            research_tbs <- lapply(seq(0,1,length.out=research_it),
                                   function(p) research_correction(tb_a_it,p))

            for (r_it in seq(research_it)){
              tb_name <- ifelse(m=="r_0","without_neighbours","with_neighbours")
              tb_name <- paste0(tb_name,
                                "_l_it_",l_it,
                                "_s_it_",s_it,
                                "_g_it_",g_it,
                                "_a_it_",a_it,
                                "_r_it_",r_it)
              
              tbs_list[[tb_name]] <- research_tbs[[r_it]]
            
              # check progress
              #print(tb_name)
            }
          }
        } 
      }
    }
  }
  return(tbs_list)
}


# this function does the same, without language bias 
all_but_language_tbs<- function(original_tb,
                                status_it = 2, # cases considered
                                gender_it = 2,
                                agricola_it = 5,
                                research_it = 5){
  # identify gu type
  gu_type <- which_gu(original_tb)
  
  # ensure that the occupation table has no zero rows
  original_tb <- original_tb[rowSums(original_tb)>0,]
  
  # initialise tb list
  tbs_list <- list()
  
  # apply status correction
  status_tbs <- status_bias_correction_stochastic(original_tb,15,30,n_samples=status_it)

  # loop and add
  for (s_it in seq_len(status_it)) {
    tb_s_it <- status_tbs[[s_it]]
    gender_tbs <- lapply(gender_ratio_v, 
                   function(gender_ratio) gender_bias(tb_s_it, gender_ratio))

    for (g_it in seq_len(gender_it)) {
      tb_g_it <- gender_tbs[[g_it]]
      agricola_tbs <- lapply(seq(0,1,length.out=agricola_it),
                     function(s) agricola_correction(tb_g_it,p=0.95,s))

      for (a_it in seq_len(agricola_it)) {
        tb_a_it <- agricola_tbs[[a_it]]
        research_tbs <- lapply(seq(0,1,length.out=research_it),
                               function(p) research_correction(tb_a_it,p))
         research_ecis <- eci_list(research_tbs)
         
         for (r_it in seq(research_it)){
              tb_name <- paste0("s_it_",s_it,
                                "_g_it_",g_it,
                                "_a_it_",a_it,
                                "_r_it_",r_it)
              
              tbs_list[[tb_name]] <- research_tbs[[r_it]]
            
              # check progress
              #print(tb_name)
              }
      }
    } 
  }
  return(tbs_list)
}
```

Apply alternative corrections to the original tables
```{r compute the eci for provinces and countries after all bias corrections}
# provinces
province_all_bias_tbs <- all_bias_tbs(province_table)
# save the results
saveRDS(province_all_bias_tbs, file = "../../results/economic_complexity/province_all_bias_tbs.rds")

# countries
country_all_bias_tbs <- all_bias_tbs(country_table)
# save the results
saveRDS(country_all_bias_tbs, file = "../../results/economic_complexity/country_all_bias_tbs.rds")
```

Apply alternative corrections but language to the filtered tables
```{r compute the eci for filtered provinces and countries after all but language bias corrections}
# provinces
filtered_province_table <- province_table[filtered_provinces,]
filtered_province_all_but_language_bias_tbs <- all_but_language_tbs(filtered_province_table)
# save the results
saveRDS(filtered_province_all_but_language_bias_tbs, file = "../../results/economic_complexity/filtered_province_all_but_language_bias_tbs.rds")

# countries
filtered_country_table <- country_table[filtered_countries,]
filtered_country_all_but_language_bias_tbs <- all_but_language_tbs(filtered_country_table)
# save the results
saveRDS(filtered_country_all_but_language_bias_tbs, file = "../../results/economic_complexity/filtered_country_all_but_language_bias_tbs.rds")
```

Plot median values of eci
```{r median eci map plots}
for (gu in c("province","country")) {
  bias_tb <- get(paste0(gu,"_individual_bias_eci_stats"))
  
  for (bias in c("language","status","gender","agricola","research")) {
    tb <- bias_tb[[bias]]
    if (bias == "language"){
      for (m in paste0("r_",max_distances)) {
        # extract eci
        eci_m <- tb[[m]]
        # extract stats
        eci_m_median <- eci_m[,c(gu,"median")]
        
        if (gu == "province"){
           map_data <- left_join(
            provinces_no_z,
            eci_m_median,
            join_by(province))
        } else if (gu == "country"){
          map_data <- left_join(
            world_sf,
            eci_m_median,
            join_by(iso3==country)) #%>% na.omit()
        }
        map_plot <- ggplot(data = map_data) +
            geom_sf(aes(fill = median)) +
             coord_sf(
                xlim = c(-15,40), 
                ylim = c(20,60)) +
            scale_fill_gradient(low = "white", high = "red", name = "Value") +
            theme_minimal() +
            labs(title = "median eci")
        
        plot_and_save(map_plot,
                      title = paste0(gu,"_",bias,"_bias_correction_",m,"_eci_median"))
      } 
    } else {
      # extract median
      eci_median <- tb[,c(gu,"median")]
      
       if (gu == "province"){
           map_data <- left_join(
            provinces_no_z,
            eci_median,
            join_by(province))
        } else if (gu == "country"){
          map_data <- left_join(
            world_sf,
            eci_median,
            join_by(iso3==country)) #%>% na.omit() 
        }
        map_plot <- ggplot(data = map_data) +
            geom_sf(aes(fill = median)) +
             coord_sf(
                xlim = c(-15,40), 
                ylim = c(20,60)) +
            scale_fill_gradient(low = "white", high = "red", name = "Value") +
            theme_minimal() +
            labs(title = "median eci")
        
        plot_and_save(map_plot,
                      title = paste0(gu,"_",bias,"_bias_correction_eci_median"))
    }
    print(bias)
  }
  print(gu)
}

# original province
 map_data <- left_join(
            provinces_no_z,
            province_individual_bias_eci_stats$original$eci,
            join_by(province))
 map_plot <- ggplot(data = map_data) +
            geom_sf(aes(fill = eci)) +
             coord_sf(
                xlim = c(-15,40), 
                ylim = c(20,60)) +
            scale_fill_gradient(low = "white", high = "red", name = "Value") +
            theme_minimal() +
            labs(title = "original eci")
   plot_and_save(map_plot,
                 title = paste0("province","_","original","_eci"))
   
# countries
map_data <- left_join(
            world_sf,
            country_individual_bias_eci_stats$original$eci,
            join_by(iso3==country)) #%>% na.omit()    
 map_plot <- ggplot(data = map_data) +
            geom_sf(aes(fill = eci)) +
             coord_sf(
                xlim = c(-15,40), 
                ylim = c(20,60)) +
            scale_fill_gradient(low = "white", high = "red", name = "Value") +
            theme_minimal() +
            labs(title = "original eci")
   plot_and_save(map_plot,
                      title = paste0("country","_","original","_eci"))
```